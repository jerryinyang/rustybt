# Story 1.4: Extend Data Pipeline with Metadata Tracking

## Status
Draft

## Story
**As a** quantitative trader,
**I want** enhanced data bundle system with metadata tracking for data provenance,
**so that** I can trace data sources, validate data quality, and understand data lineage.

## Acceptance Criteria
1. Metadata schema extended to track data source, fetch timestamp, version, and checksum for each bundle
2. Bundle ingestion records provenance metadata (source URL, API version, download time)
3. Data quality metadata stored (row count, date range, missing data gaps, outlier count)
4. Metadata queryable via Python API (e.g., `catalog.get_bundle_metadata('my_bundle')`)
5. Timezone handling improved with explicit UTC storage and conversion helpers
6. Gap detection implemented to identify missing trading days in continuous datasets
7. All metadata stored in SQLite catalog database with indexed queries
8. Tests validate metadata correctness for sample bundle ingestion

## Tasks / Subtasks
- [ ] Extend SQLite schema for metadata tracking (AC: 1, 7)
  - [ ] Create new table `bundle_metadata` with provenance fields
  - [ ] Add columns: bundle_name, source_type, source_url, api_version, fetch_timestamp, data_version, checksum (SHA256)
  - [ ] Create table `data_quality_metrics` for quality metadata
  - [ ] Add columns: bundle_name, row_count, start_date, end_date, missing_days_count, outlier_count, validation_timestamp
  - [ ] Create indexes on bundle_name, fetch_timestamp for fast queries
  - [ ] Write SQLAlchemy ORM models for new tables
  - [ ] Create database migration script to add tables to existing catalog.db

- [ ] Implement provenance tracking during bundle ingestion (AC: 2)
  - [ ] Modify bundle ingestion code to capture source metadata
  - [ ] Record source URL, API version, download timestamp during fetch
  - [ ] Calculate SHA256 checksum of raw data files
  - [ ] Store data version (e.g., API response version, file modification time)
  - [ ] Insert provenance metadata into bundle_metadata table
  - [ ] Handle incremental bundle updates (append vs. replace)

- [ ] Implement data quality metrics calculation (AC: 3)
  - [ ] Calculate row count for ingested data
  - [ ] Determine date range (min date, max date)
  - [ ] Detect missing trading days using exchange calendar
  - [ ] Identify outliers using IQR method (values >3 IQR from quartiles)
  - [ ] Validate OHLCV relationships (high >= max(open, close), low <= min(open, close))
  - [ ] Store quality metrics in data_quality_metrics table
  - [ ] Generate quality report summary

- [ ] Create metadata query API (AC: 4)
  - [ ] Implement `DataCatalog.get_bundle_metadata(bundle_name)` method
  - [ ] Return dictionary with provenance fields
  - [ ] Implement `DataCatalog.get_quality_metrics(bundle_name)` method
  - [ ] Return dictionary with quality metrics
  - [ ] Implement `DataCatalog.list_bundles()` to show all bundles with metadata
  - [ ] Add filtering options (by source, by date range)
  - [ ] Document API with examples

- [ ] Improve timezone handling (AC: 5)
  - [ ] Ensure all timestamps stored in UTC in database
  - [ ] Add timezone column to bundle_metadata table
  - [ ] Create utility functions for timezone conversion
  - [ ] Validate timestamp consistency during ingestion
  - [ ] Add explicit timezone to all pd.Timestamp objects
  - [ ] Test timezone edge cases (DST transitions, 24/7 crypto markets)

- [ ] Implement gap detection (AC: 6)
  - [ ] Create `detect_missing_days()` function using exchange calendar
  - [ ] Compare actual trading days in data with expected trading days
  - [ ] Generate list of missing dates
  - [ ] Store gap count in data_quality_metrics table
  - [ ] Create gap report showing missing date ranges
  - [ ] Add warnings for gaps exceeding threshold (e.g., >5 consecutive days)

- [ ] Add metadata to existing bundle writers (AC: 1, 2, 3)
  - [ ] Extend CSVDirBundle to record metadata
  - [ ] Extend Quandl bundle (if used) to record API metadata
  - [ ] Create base BundleWriter class with metadata hooks
  - [ ] Ensure all custom bundle writers inherit metadata tracking
  - [ ] Test metadata recording for each bundle type

- [ ] Write comprehensive tests (AC: 8)
  - [ ] Unit tests for metadata storage and retrieval
  - [ ] Unit tests for checksum calculation
  - [ ] Unit tests for data quality metrics calculation
  - [ ] Unit tests for gap detection
  - [ ] Unit tests for timezone handling
  - [ ] Integration test: Ingest sample bundle and validate metadata
  - [ ] Integration test: Query metadata API and verify correctness
  - [ ] Property-based tests using Hypothesis for edge cases

## Dev Notes

### Data Models and Schema Changes
[Source: architecture/data-models-and-schema-changes.md]

**New SQLite Tables:**

**bundle_metadata table:**
```sql
CREATE TABLE bundle_metadata (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    bundle_name TEXT NOT NULL UNIQUE,
    source_type TEXT NOT NULL,              -- 'csv', 'yfinance', 'ccxt', 'quandl', 'custom'
    source_url TEXT,                        -- API endpoint or file path
    api_version TEXT,                       -- API version if applicable
    fetch_timestamp INTEGER NOT NULL,       -- Unix timestamp (UTC)
    data_version TEXT,                      -- Version identifier from source
    checksum TEXT NOT NULL,                 -- SHA256 of raw data files
    timezone TEXT DEFAULT 'UTC',            -- Data timezone
    created_at INTEGER NOT NULL,
    updated_at INTEGER NOT NULL
);

CREATE INDEX idx_bundle_metadata_name ON bundle_metadata(bundle_name);
CREATE INDEX idx_bundle_metadata_fetch ON bundle_metadata(fetch_timestamp DESC);
```

**data_quality_metrics table:**
```sql
CREATE TABLE data_quality_metrics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    bundle_name TEXT NOT NULL,
    row_count INTEGER NOT NULL,
    start_date INTEGER NOT NULL,            -- Unix timestamp
    end_date INTEGER NOT NULL,              -- Unix timestamp
    missing_days_count INTEGER DEFAULT 0,
    missing_days_list TEXT,                 -- JSON array of missing dates
    outlier_count INTEGER DEFAULT 0,
    ohlcv_violations INTEGER DEFAULT 0,     -- Rows violating OHLCV relationships
    validation_timestamp INTEGER NOT NULL,
    validation_passed BOOLEAN DEFAULT 1,
    FOREIGN KEY(bundle_name) REFERENCES bundle_metadata(bundle_name)
);

CREATE INDEX idx_quality_metrics_bundle ON data_quality_metrics(bundle_name);
CREATE INDEX idx_quality_metrics_validation ON data_quality_metrics(validation_timestamp DESC);
```

**SQLAlchemy ORM Models:**
```python
from sqlalchemy import Column, Integer, Text, Boolean, ForeignKey
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class BundleMetadata(Base):
    __tablename__ = 'bundle_metadata'

    id = Column(Integer, primary_key=True)
    bundle_name = Column(Text, nullable=False, unique=True)
    source_type = Column(Text, nullable=False)
    source_url = Column(Text)
    api_version = Column(Text)
    fetch_timestamp = Column(Integer, nullable=False)
    data_version = Column(Text)
    checksum = Column(Text, nullable=False)
    timezone = Column(Text, default='UTC')
    created_at = Column(Integer, nullable=False)
    updated_at = Column(Integer, nullable=False)

class DataQualityMetrics(Base):
    __tablename__ = 'data_quality_metrics'

    id = Column(Integer, primary_key=True)
    bundle_name = Column(Text, ForeignKey('bundle_metadata.bundle_name'), nullable=False)
    row_count = Column(Integer, nullable=False)
    start_date = Column(Integer, nullable=False)
    end_date = Column(Integer, nullable=False)
    missing_days_count = Column(Integer, default=0)
    missing_days_list = Column(Text)  # JSON
    outlier_count = Column(Integer, default=0)
    ohlcv_violations = Column(Integer, default=0)
    validation_timestamp = Column(Integer, nullable=False)
    validation_passed = Column(Boolean, default=True)
```

### Data Pipeline Extension Points
[Source: architecture/existing-project-analysis.md#data-flow]

**Zipline Bundle Architecture:**
- Bundles located in `data/bundles/` directory
- Core bundle writer: `data/bundles/core.py`
- CSV bundle: `data/bundles/csvdir.py`
- Bundle ingestion: `zipline ingest -b bundle_name` CLI command

**Integration Points:**
- Modify `BundleData` class to record metadata
- Hook into `ingest()` function to trigger metadata recording
- Extend `BundleWriter` base class with metadata methods
- Store metadata in catalog.db (same database as asset metadata)

### Source Tree
[Source: architecture/source-tree.md]

**Files to Modify:**
- `rustybt/data/bundles/core.py`: Extend BundleWriter with metadata hooks
- `rustybt/data/bundles/csvdir.py`: Add metadata recording to CSV bundle
- `rustybt/assets/asset_db_schema.py`: Add new tables to schema

**Files to Create:**
- `rustybt/data/catalog.py`: DataCatalog class for metadata queries
- `rustybt/data/quality.py`: Data quality metrics calculation
- `rustybt/utils/checksum.py`: Checksum calculation utilities
- `rustybt/utils/gap_detection.py`: Gap detection utilities

### Tech Stack
[Source: architecture/tech-stack.md]

**Database:**
- SQLite: 3.x (existing Zipline database)
- SQLAlchemy: >= 2.0 (existing ORM)
- Store all financial values as TEXT (Decimal representation)
- Use INTEGER for timestamps (Unix time in seconds or milliseconds)

**Data Validation:**
- Polars: For fast data quality checks
- exchange-calendars: For trading calendar validation

**Hashing:**
- hashlib (stdlib): SHA256 checksum calculation

### Coding Standards
[Source: architecture/coding-standards.md]

**Database Operations:**
- Use SQLAlchemy ORM for all database operations
- Parameterized queries to prevent SQL injection
- Transaction management: Use context managers for commits/rollbacks

**Metadata Storage:**
- All timestamps in UTC
- Store Decimal values as TEXT to preserve precision
- JSON for complex data structures (missing_days_list)

**Type Hints:**
- 100% type hint coverage for metadata API
- Example:
```python
from typing import Dict, List, Optional
from decimal import Decimal
import pandas as pd

def get_bundle_metadata(bundle_name: str) -> Optional[Dict[str, Any]]:
    """Get metadata for bundle."""
    pass

def calculate_quality_metrics(
    data: pl.DataFrame,
    calendar: TradingCalendar
) -> Dict[str, Any]:
    """Calculate data quality metrics."""
    pass
```

### Zero-Mock Enforcement
[Source: architecture/coding-standards.md#zero-mock-enforcement-mandatory]

**Real Implementations Required:**
- Actual SHA256 checksum calculation (not hardcoded)
- Real data quality validation (not always returning True)
- Actual gap detection using real calendar (not fake missing days)
- Real database queries (not mocked results)

**Validation Requirements:**
- OHLCV relationship validation must check actual data
- Outlier detection must use statistical methods
- Gap detection must use real trading calendar

### Testing Strategy
[Source: architecture/testing-strategy.md]

**Unit Tests:**
```python
def test_bundle_metadata_storage():
    """Test metadata correctly stored and retrieved."""
    metadata = {
        'bundle_name': 'test_bundle',
        'source_type': 'csv',
        'source_url': '/data/test.csv',
        'checksum': calculate_checksum('/data/test.csv'),
        'fetch_timestamp': int(time.time())
    }
    catalog.store_metadata(metadata)
    retrieved = catalog.get_bundle_metadata('test_bundle')
    assert retrieved['checksum'] == metadata['checksum']

def test_checksum_calculation():
    """Test SHA256 checksum calculation."""
    # Create temp file with known content
    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:
        f.write('test data')
        filepath = f.name

    checksum = calculate_checksum(filepath)
    # Verify checksum is valid SHA256 (64 hex chars)
    assert len(checksum) == 64
    assert all(c in '0123456789abcdef' for c in checksum)

    os.unlink(filepath)

def test_data_quality_metrics():
    """Test quality metrics calculation."""
    # Create sample data with known issues
    data = pl.DataFrame({
        'date': pd.date_range('2023-01-01', '2023-01-10'),
        'open': [100.0, 101.0, 99.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0],
        'high': [105.0, 106.0, 104.0, 107.0, 108.0, 109.0, 110.0, 111.0, 112.0, 113.0],
        'low': [98.0, 99.0, 97.0, 100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0],
        'close': [102.0, 100.0, 101.0, 105.0, 104.0, 106.0, 107.0, 108.0, 109.0, 110.0],
        'volume': [1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900]
    })

    metrics = calculate_quality_metrics(data, calendar=NYSE())
    assert metrics['row_count'] == 10
    assert metrics['start_date'] == pd.Timestamp('2023-01-01')
    assert metrics['end_date'] == pd.Timestamp('2023-01-10')

def test_gap_detection():
    """Test missing trading days detection."""
    # Create data with missing day
    dates = pd.date_range('2023-01-03', '2023-01-10', freq='B')  # Business days
    dates = dates.drop(pd.Timestamp('2023-01-05'))  # Remove one day

    data = pl.DataFrame({'date': dates, 'close': [100.0] * len(dates)})
    gaps = detect_missing_days(data, calendar=NYSE())

    assert len(gaps) == 1
    assert gaps[0] == pd.Timestamp('2023-01-05')
```

**Integration Tests:**
```python
@pytest.mark.integration
def test_csv_bundle_with_metadata():
    """Test CSV bundle ingestion records metadata."""
    bundle_name = 'test_csv_bundle'

    # Ingest bundle
    ingest(bundle_name, show_progress=False)

    # Verify metadata stored
    catalog = DataCatalog()
    metadata = catalog.get_bundle_metadata(bundle_name)

    assert metadata is not None
    assert metadata['bundle_name'] == bundle_name
    assert metadata['source_type'] == 'csv'
    assert metadata['checksum'] is not None
    assert len(metadata['checksum']) == 64  # SHA256

    # Verify quality metrics
    quality = catalog.get_quality_metrics(bundle_name)
    assert quality['row_count'] > 0
    assert quality['start_date'] < quality['end_date']
```

**Property-Based Tests:**
```python
from hypothesis import given, strategies as st

@given(
    dates=st.lists(
        st.datetimes(min_value=datetime(2020, 1, 1), max_value=datetime(2023, 12, 31)),
        min_size=10,
        max_size=100,
        unique=True
    )
)
def test_gap_detection_invariant(dates):
    """Gap detection should find all missing trading days."""
    sorted_dates = sorted(dates)
    data = pl.DataFrame({'date': sorted_dates, 'close': [100.0] * len(sorted_dates)})

    gaps = detect_missing_days(data, calendar=NYSE())

    # All gaps should be valid trading days
    for gap in gaps:
        assert calendar.is_session(gap)
```

**Test Coverage Target:**
- Unit tests: â‰¥95% coverage for metadata and quality modules
- Integration tests: Test all bundle types with metadata recording
- Property tests: 1000+ examples for gap detection and validation

### API Examples

**Metadata Query API:**
```python
from rustybt.data.catalog import DataCatalog

catalog = DataCatalog()

# Get bundle metadata
metadata = catalog.get_bundle_metadata('my_bundle')
print(f"Source: {metadata['source_type']}")
print(f"Fetched: {pd.Timestamp(metadata['fetch_timestamp'], unit='s')}")
print(f"Checksum: {metadata['checksum']}")

# Get quality metrics
quality = catalog.get_quality_metrics('my_bundle')
print(f"Rows: {quality['row_count']}")
print(f"Date range: {quality['start_date']} to {quality['end_date']}")
print(f"Missing days: {quality['missing_days_count']}")
print(f"Outliers: {quality['outlier_count']}")

# List all bundles
bundles = catalog.list_bundles()
for bundle in bundles:
    print(f"{bundle['bundle_name']}: {bundle['source_type']} ({bundle['row_count']} rows)")
```

### Testing

**Test File Location:**
- Unit tests: `tests/data/test_catalog.py`, `tests/data/test_quality.py`
- Integration tests: `tests/data/test_bundle_metadata_integration.py`
- Utilities tests: `tests/utils/test_checksum.py`, `tests/utils/test_gap_detection.py`

**Test Standards:**
- Test metadata storage and retrieval
- Test checksum calculation with real files
- Test data quality metrics with known datasets
- Test gap detection with real trading calendars
- Test timezone handling with UTC and local timezones
- Integration test with CSV bundle ingestion

**Testing Frameworks:**
- pytest for test framework
- hypothesis for property-based testing
- tempfile for temporary test files
- SQLite in-memory database for testing

**Manual Verification:**
1. Ingest sample CSV bundle
2. Query metadata via Python API
3. Verify checksum matches expected value
4. Verify quality metrics are accurate
5. Test gap detection with intentionally incomplete data
6. Verify all metadata stored in database

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-30 | 1.0 | Initial story creation | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent_

### Debug Log References
_To be populated by dev agent_

### Completion Notes List
_To be populated by dev agent_

### File List
_To be populated by dev agent_

## QA Results
_To be populated by QA agent_
