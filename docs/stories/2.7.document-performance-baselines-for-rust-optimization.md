# Story 2.7: Document Performance Baselines for Rust Optimization

## Status
Approved for development

## Story
**As a** developer,
**I want** comprehensive performance benchmarks comparing Decimal vs. float implementations,
**so that** Epic 7 (Rust optimization) has clear targets for optimization efforts.

## Acceptance Criteria
1. Benchmark suite created using pytest-benchmark or timeit
2. Baseline measured: typical backtest with float (pre-Epic 2)
3. Post-Decimal measured: same backtest with Decimal implementation
4. Overhead calculated: (Decimal_time / float_time - 1) Ã— 100%
5. Per-module overhead measured: calculation engine, order execution, metrics, data pipeline
6. Memory overhead measured: Decimal vs. float memory consumption
7. Hotspot profiling performed: identify top 10 time-consuming functions with Decimal
8. Benchmark results documented in docs/performance/decimal-baseline.md
9. CI/CD integration: benchmarks run on every release to track regression
10. Target established: Epic 7 must bring overhead to <30% vs. float baseline

## Tasks / Subtasks

- [ ] Create benchmark infrastructure (AC: 1)
  - [ ] Add pytest-benchmark to dependencies (â‰¥3.4.1)
  - [ ] Create `benchmarks/` directory for all benchmark files
  - [ ] Create pytest benchmark configuration in `pyproject.toml`
  - [ ] Set benchmark iterations: 10 rounds, 100 iterations per round
  - [ ] Enable benchmark statistics: mean, median, std, min, max
  - [ ] Enable benchmark calibration: auto-adjust iterations for accuracy
  - [ ] Configure benchmark storage: save results to JSON
  - [ ] Create benchmark comparison script: compare Decimal vs float
  - [ ] Configure CI/CD benchmark job: run on release branches
  - [ ] Document benchmark methodology in `docs/performance/benchmarking.md`

- [ ] Implement float baseline backtest (AC: 2)
  - [ ] Create `benchmarks/baseline_float_backtest.py`
  - [ ] Implement minimal backtest using original Zipline float code
  - [ ] Strategy: simple moving average crossover (20/50 day)
  - [ ] Data: 1 year of daily bars for 100 assets
  - [ ] Measure end-to-end execution time
  - [ ] Measure peak memory usage using memory_profiler
  - [ ] Run 10 iterations and report mean execution time
  - [ ] Document float baseline results: execution time, memory usage
  - [ ] Save float baseline for comparison: `benchmarks/results/float_baseline.json`
  - [ ] Use this as 100% reference point for overhead calculation

- [ ] Implement Decimal backtest benchmark (AC: 3)
  - [ ] Create `benchmarks/decimal_backtest.py`
  - [ ] Implement same backtest using Decimal implementations
  - [ ] Use DecimalLedger, DecimalOrder, DecimalTransaction from Stories 2.2-2.3
  - [ ] Use Decimal data pipeline from Story 2.5
  - [ ] Use Decimal metrics from Story 2.4
  - [ ] Same strategy, same data as float baseline
  - [ ] Measure end-to-end execution time
  - [ ] Measure peak memory usage
  - [ ] Run 10 iterations and report mean execution time
  - [ ] Save Decimal results: `benchmarks/results/decimal_backtest.json`

- [ ] Calculate overall performance overhead (AC: 4)
  - [ ] Create `benchmarks/calculate_overhead.py` script
  - [ ] Load float baseline results from JSON
  - [ ] Load Decimal benchmark results from JSON
  - [ ] Calculate time overhead: (Decimal_time / float_time - 1) Ã— 100%
  - [ ] Calculate memory overhead: (Decimal_memory / float_memory - 1) Ã— 100%
  - [ ] Generate comparison report: overhead percentages per metric
  - [ ] Visualize results: bar chart comparing float vs Decimal
  - [ ] Export report to Markdown: `docs/performance/decimal-baseline.md`
  - [ ] Include statistics: mean, median, std dev, confidence intervals
  - [ ] Document overhead findings and implications

- [ ] Benchmark DecimalLedger performance (AC: 5)
  - [ ] Create `benchmarks/decimal_ledger_benchmark.py`
  - [ ] Benchmark: calculate_portfolio_value() for 100 positions
  - [ ] Benchmark: calculate_returns() for 252 trading days
  - [ ] Benchmark: process_transaction() for 1000 transactions
  - [ ] Compare with float-based Ledger (if available)
  - [ ] Measure per-operation overhead: microseconds per calculation
  - [ ] Identify bottlenecks: which operations are slowest?
  - [ ] Test scalability: benchmark with 10, 100, 1000, 10000 positions
  - [ ] Plot performance curve: operations/sec vs. portfolio size
  - [ ] Document DecimalLedger overhead: typically 20-40% vs float

- [ ] Benchmark DecimalOrder execution (AC: 5)
  - [ ] Create `benchmarks/decimal_order_benchmark.py`
  - [ ] Benchmark: order submission and validation (1000 orders)
  - [ ] Benchmark: order fill calculation with commission/slippage
  - [ ] Benchmark: partial fill tracking and average price calculation
  - [ ] Benchmark: order value calculation (price Ã— quantity)
  - [ ] Compare with float-based order processing
  - [ ] Measure per-order overhead: microseconds per order
  - [ ] Test with various order sizes: 1 share to 10000 shares
  - [ ] Test with fractional quantities (crypto precision)
  - [ ] Document DecimalOrder overhead: typically 15-30% vs float

- [ ] Benchmark Decimal metrics calculation (AC: 5)
  - [ ] Create `benchmarks/decimal_metrics_benchmark.py`
  - [ ] Benchmark: Sharpe ratio calculation (252 returns)
  - [ ] Benchmark: Maximum drawdown calculation (252 returns)
  - [ ] Benchmark: VaR/CVaR calculation (1000 returns)
  - [ ] Benchmark: Performance attribution (20 positions)
  - [ ] Compare with float-based metrics (empyrical-reloaded)
  - [ ] Measure per-metric overhead: milliseconds per calculation
  - [ ] Test with varying return series lengths: 100, 1000, 10000
  - [ ] Identify slowest metric: likely max_drawdown or CVaR
  - [ ] Document Decimal metrics overhead: typically 25-50% vs float

- [ ] Benchmark Decimal data pipeline (AC: 5)
  - [ ] Create `benchmarks/decimal_data_pipeline_benchmark.py`
  - [ ] Benchmark: load 1 year daily bars (252 days, 100 assets) from Parquet
  - [ ] Benchmark: load 1 day minute bars (390 bars, 100 assets) from Parquet
  - [ ] Benchmark: resample minute â†’ daily (1 month of minute data)
  - [ ] Benchmark: apply split adjustment to 10 years of data
  - [ ] Benchmark: OHLCV validation for 10000 bars
  - [ ] Compare with float-based data loading (HDF5/bcolz)
  - [ ] Measure data loading overhead: seconds per year of data
  - [ ] Test with different compression: SNAPPY, ZSTD, LZ4
  - [ ] Identify bottleneck: likely Parquet read or type casting
  - [ ] Document data pipeline overhead: typically 10-25% vs float

- [ ] Measure memory overhead (AC: 6)
  - [ ] Create `benchmarks/memory_overhead_benchmark.py`
  - [ ] Use memory_profiler to measure peak memory usage
  - [ ] Benchmark: Decimal vs float for 1000 positions in memory
  - [ ] Benchmark: Decimal vs float for 252 days of returns series
  - [ ] Benchmark: Decimal vs float for 1 year daily bars (100 assets)
  - [ ] Calculate memory overhead: bytes per Decimal vs float64
  - [ ] Expected: Decimal ~2-3x larger than float64 (128-bit vs 64-bit)
  - [ ] Measure with Polars DataFrame: Decimal columns vs Float64 columns
  - [ ] Test memory scaling: 100 assets, 1000 assets, 10000 assets
  - [ ] Document memory overhead: typically 100-150% (2-2.5x) vs float

- [ ] Profile hotspots with cProfile (AC: 7)
  - [ ] Create `benchmarks/profile_decimal_backtest.py`
  - [ ] Run Decimal backtest with cProfile enabled
  - [ ] Profile entire backtest execution
  - [ ] Generate profile statistics: `python -m cProfile -o profile.stats`
  - [ ] Analyze with pstats or snakeviz visualization
  - [ ] Identify top 10 time-consuming functions
  - [ ] Expected hotspots: Decimal arithmetic, Polars operations, commission calculation
  - [ ] Calculate % time spent in each hotspot
  - [ ] Prioritize optimization targets for Epic 7 (Rust)
  - [ ] Document profiling results with call graphs and flame charts
  - [ ] Export top 10 hotspots table to Markdown

- [ ] Create performance baseline documentation (AC: 8)
  - [ ] Create `docs/performance/decimal-baseline.md`
  - [ ] Section: Executive Summary (overhead percentages)
  - [ ] Section: Benchmarking Methodology (setup, metrics, iterations)
  - [ ] Section: Overall Performance (float vs Decimal end-to-end)
  - [ ] Section: Per-Module Breakdown (ledger, orders, metrics, data)
  - [ ] Section: Memory Overhead (memory usage comparison)
  - [ ] Section: Hotspot Analysis (top 10 functions)
  - [ ] Section: Scalability Testing (performance vs. portfolio size)
  - [ ] Section: Optimization Targets (priorities for Epic 7)
  - [ ] Include charts: bar charts, line graphs, flame charts
  - [ ] Include tables: benchmark statistics, overhead calculations
  - [ ] Document Epic 7 goal: reduce overhead to <30%

- [ ] Integrate benchmarks into CI/CD (AC: 9)
  - [ ] Create `.github/workflows/benchmarks.yml`
  - [ ] Trigger: on release tags, manual workflow dispatch
  - [ ] Run all benchmark suites on dedicated runner (consistent hardware)
  - [ ] Save benchmark results as workflow artifacts
  - [ ] Compare with previous release baseline
  - [ ] Fail if performance regresses >10% without justification
  - [ ] Generate benchmark comparison report in PR comments
  - [ ] Store historical benchmark data in repository
  - [ ] Create benchmark dashboard: track performance over time
  - [ ] Schedule weekly benchmark runs on main branch

- [ ] Establish Epic 7 optimization targets (AC: 10)
  - [ ] Analyze Decimal overhead per module
  - [ ] Set Epic 7 target: overall overhead <30% vs float
  - [ ] Module targets:
    - DecimalLedger: reduce from 30% to <15%
    - DecimalOrder: reduce from 25% to <10%
    - Decimal metrics: reduce from 40% to <20%
    - Data pipeline: reduce from 20% to <10%
  - [ ] Prioritize Rust optimization candidates:
    - Priority 1: Decimal arithmetic operations (hottest path)
    - Priority 2: Metrics calculations (computationally intensive)
    - Priority 3: Data aggregation (large data volumes)
  - [ ] Document optimization strategy in `docs/architecture/epic-7-rust-optimization-plan.md`
  - [ ] Create Epic 7 success criteria based on benchmarks
  - [ ] Validate targets are achievable with Rust (research similar projects)

## Dev Notes

### Previous Story Insights
[Dependency: Story 2.1 - Design Decimal Precision Configuration System]
[Dependency: Story 2.2 - Replace Float with Decimal in Core Calculation Engine]
[Dependency: Story 2.3 - Replace Float with Decimal in Order Execution System]
[Dependency: Story 2.4 - Replace Float with Decimal in Performance Metrics]
[Dependency: Story 2.5 - Replace Float with Decimal in Data Pipelines]
[Dependency: Story 2.6 - Implement Property-Based Testing for Financial Calculations]

**Integration Requirements:**
This story benchmarks ALL implementations from Stories 2.1-2.6:
- Story 2.1: DecimalConfig (minimal overhead, configuration only)
- Story 2.2: DecimalLedger (moderate overhead, arithmetic intensive)
- Story 2.3: DecimalOrder/Transaction (moderate overhead, per-order processing)
- Story 2.4: Decimal metrics (high overhead, statistical calculations)
- Story 2.5: Decimal data pipeline (moderate overhead, I/O and type conversion)
- Story 2.6: Property-based tests (not benchmarked, testing only)

**Purpose:**
Establish performance baseline for Epic 7 Rust optimization. Benchmarks identify:
1. Current overhead of Decimal vs float implementations
2. Performance bottlenecks (hotspots) to prioritize for Rust optimization
3. Scalability characteristics (how overhead changes with data size)
4. Memory impact of Decimal (for capacity planning)
5. Regression tracking (ensure no performance degradation over time)

**Design Decisions:**
- Use pytest-benchmark for consistent, statistical benchmarking
- Run benchmarks on dedicated CI/CD runner for reproducibility
- Store historical results for trend analysis
- Set Epic 7 target at <30% overhead (aggressive but achievable with Rust)
- Prioritize Rust optimization based on hotspot profiling results

### Component Architecture
[Source: architecture/component-architecture.md]

**Performance Benchmarking Strategy:**
- End-to-end backtest: measures total system performance
- Per-module benchmarks: isolate overhead sources
- Micro-benchmarks: measure individual operations (portfolio_value, order_fill)
- Scalability tests: measure performance vs. data size
- Memory profiling: track memory overhead separately from CPU overhead

### Tech Stack
[Source: architecture/tech-stack.md]

**Benchmarking Tools:**
- **pytest-benchmark** (â‰¥3.4.1): Statistical benchmarking framework
  - Automatic warmup and calibration
  - Statistical analysis (mean, median, std dev)
  - Comparison with baseline results
  - JSON export for historical tracking

- **cProfile** (stdlib): Python profiler
  - Function-level profiling
  - Call count and cumulative time tracking
  - Integration with snakeviz for visualization

- **memory_profiler** (â‰¥0.61.0): Memory usage tracking
  - Line-by-line memory profiling
  - Peak memory usage measurement
  - Memory timeline visualization

- **snakeviz** (â‰¥2.2.0): Profile visualization
  - Interactive call graph
  - Flame chart visualization
  - Exportable SVG/PNG charts

**CI/CD Tools:**
- **GitHub Actions**: Automated benchmark execution
- **Benchmark dashboard**: Historical tracking (custom or GitHub Pages)

### Source Tree
[Source: architecture/source-tree.md]

**New Files to Create:**
```
benchmarks/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py                       # pytest-benchmark configuration
â”œâ”€â”€ baseline_float_backtest.py        # Float baseline benchmark
â”œâ”€â”€ decimal_backtest.py               # Decimal backtest benchmark
â”œâ”€â”€ decimal_ledger_benchmark.py       # DecimalLedger benchmarks
â”œâ”€â”€ decimal_order_benchmark.py        # DecimalOrder benchmarks
â”œâ”€â”€ decimal_metrics_benchmark.py      # Metrics calculation benchmarks
â”œâ”€â”€ decimal_data_pipeline_benchmark.py # Data pipeline benchmarks
â”œâ”€â”€ memory_overhead_benchmark.py      # Memory usage benchmarks
â”œâ”€â”€ profile_decimal_backtest.py       # cProfile profiling script
â”œâ”€â”€ calculate_overhead.py             # Overhead calculation script
â””â”€â”€ results/                          # Benchmark results storage
    â”œâ”€â”€ float_baseline.json
    â”œâ”€â”€ decimal_backtest.json
    â””â”€â”€ ...
```

**Documentation Files:**
```
docs/performance/
â”œâ”€â”€ benchmarking.md                   # Benchmarking methodology
â”œâ”€â”€ decimal-baseline.md               # Performance baseline results (NEW)
â””â”€â”€ epic-7-rust-optimization-plan.md  # Rust optimization strategy (NEW)
```

**CI/CD Workflows:**
```
.github/workflows/
â””â”€â”€ benchmarks.yml                    # Benchmark automation (NEW)
```

### Coding Standards
[Source: architecture/coding-standards.md]

**pytest-benchmark Usage:**
```python
import pytest
from decimal import Decimal
from rustybt.finance.decimal import DecimalLedger, DecimalPosition
from rustybt.assets import Equity

@pytest.fixture
def large_portfolio():
    """Create portfolio with 100 positions for benchmarking."""
    ledger = DecimalLedger(starting_cash=Decimal("1000000"))

    for i in range(100):
        asset = Equity(sid=i, symbol=f"STOCK{i}")
        position = DecimalPosition(
            asset=asset,
            amount=Decimal("100"),
            cost_basis=Decimal("50"),
            last_sale_price=Decimal("55")
        )
        ledger.positions[asset] = position

    return ledger

def test_portfolio_value_benchmark(benchmark, large_portfolio):
    """Benchmark portfolio value calculation with 100 positions.

    Baseline target: <1ms per calculation
    Epic 7 target: <0.5ms per calculation (Rust optimization)
    """
    result = benchmark(lambda: large_portfolio.portfolio_value)

    # Verify result is correct
    assert result > Decimal("0")

    # Optional: assert performance target (warning, not failure)
    stats = benchmark.stats
    if stats['mean'] > 0.001:  # 1ms
        pytest.warn(
            f"Portfolio value calculation slower than target: "
            f"{stats['mean']:.4f}s > 0.001s"
        )
```

**cProfile Profiling:**
```python
import cProfile
import pstats
from rustybt.algorithm import TradingAlgorithm
from rustybt.data.polars import PolarsDataPortal

def profile_decimal_backtest():
    """Profile Decimal backtest execution to identify hotspots."""
    profiler = cProfile.Profile()

    # Run backtest with profiling
    profiler.enable()
    run_decimal_backtest()  # Your backtest function
    profiler.disable()

    # Save profile statistics
    profiler.dump_stats('benchmarks/results/decimal_backtest.prof')

    # Print top 20 functions by cumulative time
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(20)

    # Export for visualization
    # Use: snakeviz benchmarks/results/decimal_backtest.prof

if __name__ == "__main__":
    profile_decimal_backtest()
```

**Memory Profiling:**
```python
from memory_profiler import profile
from decimal import Decimal
import polars as pl

@profile
def benchmark_memory_overhead():
    """Measure memory overhead of Decimal vs float.

    Expected:
        Decimal: ~16 bytes per value (128-bit)
        float64: ~8 bytes per value (64-bit)
        Overhead: ~100% (2x memory usage)
    """
    # Create large Decimal series
    decimal_values = [Decimal(f"{i}.{i:02d}") for i in range(100000)]
    decimal_series = pl.Series("decimal", decimal_values)

    # Create equivalent float series
    float_values = [float(f"{i}.{i:02d}") for i in range(100000)]
    float_series = pl.Series("float", float_values)

    # Memory is measured automatically by @profile decorator
    # Run with: python -m memory_profiler benchmark_script.py

    return decimal_series, float_series
```

**Benchmark Configuration:**
```toml
# pyproject.toml
[tool.pytest.ini_options]
addopts = [
    "--benchmark-only",           # Only run benchmarks (skip tests)
    "--benchmark-autosave",       # Auto-save results
    "--benchmark-storage=benchmarks/results",  # Storage location
    "--benchmark-compare",        # Compare with previous run
    "--benchmark-columns=mean,median,stddev,rounds,iterations",
    "--benchmark-sort=name",
]

[tool.pytest-benchmark]
min_rounds = 10
min_time = 0.1
max_time = 10.0
calibration_precision = 10
warmup = true
warmup_iterations = 5
```

### Zero-Mock Enforcement
[Source: architecture/coding-standards.md#zero-mock-enforcement-mandatory]

**Real Implementations Required:**
- Benchmarks must measure actual implementations (not mocked functions)
- Benchmarks must use real data (not simplified/synthetic edge cases)
- Benchmarks must perform actual calculations (not return cached results)
- Overhead calculations must use actual timing measurements (not estimates)

**Forbidden Patterns:**
```python
# âŒ ABSOLUTELY FORBIDDEN
def test_portfolio_value_benchmark(benchmark):
    # Benchmarking trivial operation (not real implementation)
    result = benchmark(lambda: Decimal("100000"))
    assert result == Decimal("100000")

def test_fake_overhead_calculation():
    # Not measuring actual overhead, just hardcoded
    float_time = 1.0
    decimal_time = 1.3
    overhead = 30  # Hardcoded 30% overhead
    assert overhead == 30

# Using cached results instead of re-calculating
cached_portfolio_value = Decimal("100000")
def test_cached_benchmark(benchmark):
    result = benchmark(lambda: cached_portfolio_value)

# âœ… CORRECT IMPLEMENTATION
def test_portfolio_value_benchmark(benchmark, large_portfolio):
    """Benchmark actual portfolio value calculation."""
    # Benchmark real calculation (not cached result)
    result = benchmark(lambda: large_portfolio.portfolio_value)

    # Verify result is correct (not hardcoded)
    assert result > Decimal("0")

    # Result changes if portfolio changes
    large_portfolio.cash += Decimal("1000")
    new_result = large_portfolio.portfolio_value
    assert new_result != result  # Proves we're calculating, not caching

def test_actual_overhead_calculation():
    """Calculate actual overhead from real measurements."""
    # Run float baseline
    float_results = []
    for _ in range(10):
        start = time.perf_counter()
        run_float_backtest()
        end = time.perf_counter()
        float_results.append(end - start)

    # Run Decimal backtest
    decimal_results = []
    for _ in range(10):
        start = time.perf_counter()
        run_decimal_backtest()
        end = time.perf_counter()
        decimal_results.append(end - start)

    # Calculate actual overhead from measurements
    float_mean = sum(float_results) / len(float_results)
    decimal_mean = sum(decimal_results) / len(decimal_results)
    overhead = (decimal_mean / float_mean - 1) * 100

    # Overhead is measured, not assumed
    assert overhead > 0, "Decimal should have some overhead vs float"
```

### Testing Strategy

**Benchmark Test Examples:**

```python
# End-to-End Backtest Benchmark
def test_decimal_backtest_performance(benchmark):
    """Benchmark complete Decimal backtest execution.

    Scenario:
        - 1 year of daily data (252 bars)
        - 100 assets
        - Simple moving average crossover strategy
        - ~500 orders executed

    Baseline (float): ~2.5 seconds
    Current (Decimal): ~3.2 seconds (28% overhead)
    Epic 7 Target (Rust): <2.8 seconds (<12% overhead)
    """
    def run_backtest():
        from rustybt.algorithm import TradingAlgorithm
        from rustybt.data.polars import PolarsDataPortal

        algo = TradingAlgorithm(...)
        result = algo.run(start_date, end_date)
        return result

    result = benchmark(run_backtest)
    assert result is not None

# Scalability Benchmark
@pytest.mark.parametrize("num_positions", [10, 100, 1000, 10000])
def test_portfolio_value_scalability(benchmark, num_positions):
    """Test portfolio value calculation scales with position count.

    Expected:
        O(n) complexity - linear scaling with positions
        10 positions: ~0.1ms
        100 positions: ~1ms
        1000 positions: ~10ms
        10000 positions: ~100ms
    """
    ledger = create_portfolio_with_positions(num_positions)

    result = benchmark(lambda: ledger.portfolio_value)

    # Log scaling characteristics
    print(f"Portfolio value calc for {num_positions} positions: "
          f"{benchmark.stats['mean']:.6f}s")

# Memory Overhead Measurement
def test_memory_overhead():
    """Measure memory overhead of Decimal vs float.

    Uses memory_profiler to track peak memory usage.
    """
    import tracemalloc

    # Measure Decimal memory
    tracemalloc.start()
    decimal_data = create_decimal_portfolio(10000)
    decimal_current, decimal_peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # Measure float memory
    tracemalloc.start()
    float_data = create_float_portfolio(10000)
    float_current, float_peak = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # Calculate overhead
    memory_overhead = (decimal_peak / float_peak - 1) * 100

    print(f"Decimal memory: {decimal_peak / 1024 / 1024:.2f} MB")
    print(f"Float memory: {float_peak / 1024 / 1024:.2f} MB")
    print(f"Memory overhead: {memory_overhead:.1f}%")

    # Expected: ~100-150% overhead (2-2.5x)
    assert 80 <= memory_overhead <= 200

# Hotspot Identification
def test_identify_hotspots():
    """Profile Decimal backtest and identify optimization targets."""
    import cProfile
    import pstats

    profiler = cProfile.Profile()
    profiler.enable()

    # Run Decimal backtest
    run_decimal_backtest()

    profiler.disable()

    # Analyze results
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')

    # Get top 10 functions by cumulative time
    stats_list = stats.get_stats_profile()
    top_functions = sorted(
        stats_list.items(),
        key=lambda x: x[1].cumulative_time,
        reverse=True
    )[:10]

    # Document hotspots
    print("Top 10 Hotspots for Rust Optimization:")
    for func, stats in top_functions:
        print(f"  {func}: {stats.cumulative_time:.3f}s "
              f"({stats.cumulative_time / total_time * 100:.1f}%)")

    # Export for Epic 7 planning
    with open("docs/performance/hotspots.md", "w") as f:
        f.write("# Decimal Implementation Hotspots\n\n")
        for func, stats in top_functions:
            f.write(f"- `{func}`: {stats.cumulative_time:.3f}s\n")
```

**CI/CD Benchmark Workflow:**
```yaml
# .github/workflows/benchmarks.yml
name: Performance Benchmarks

on:
  release:
    types: [published]
  workflow_dispatch:
  schedule:
    - cron: '0 3 * * 1'  # Weekly Monday 3am

jobs:
  benchmarks:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          pip install uv
          uv sync --extra benchmarks

      - name: Run benchmarks
        run: |
          pytest benchmarks/ \
            --benchmark-only \
            --benchmark-autosave \
            --benchmark-compare \
            --benchmark-json=benchmark_results.json

      - name: Calculate overhead
        run: |
          python benchmarks/calculate_overhead.py \
            benchmarks/results/float_baseline.json \
            benchmark_results.json \
            --output docs/performance/decimal-baseline.md

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            benchmark_results.json
            docs/performance/decimal-baseline.md

      - name: Check performance regression
        run: |
          python benchmarks/check_regression.py \
            --threshold 10 \
            --fail-on-regression

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const results = fs.readFileSync('docs/performance/decimal-baseline.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## Performance Benchmark Results\n\n${results}`
            });
```

**Documentation Template (decimal-baseline.md):**
```markdown
# Decimal Performance Baseline

**Generated:** 2025-10-01
**Benchmark Version:** 1.0
**Hardware:** GitHub Actions Ubuntu Runner

## Executive Summary

- **Overall Overhead:** 28.3% slower than float baseline
- **Memory Overhead:** 127% larger than float (2.27x memory)
- **Epic 7 Target:** Reduce overhead to <30% (ACHIEVED) â†’ <12% (AGGRESSIVE)

## Benchmarking Methodology

- **Framework:** pytest-benchmark v3.4.1
- **Iterations:** 10 rounds Ã— 100 iterations per benchmark
- **Calibration:** Automatic warmup and calibration
- **Data:** 1 year daily bars, 100 assets, 500 orders

## Overall Performance

| Metric | Float Baseline | Decimal Current | Overhead |
|--------|----------------|-----------------|----------|
| End-to-end backtest | 2.487s | 3.191s | +28.3% |
| Peak memory | 145 MB | 329 MB | +127% |

## Per-Module Breakdown

### DecimalLedger
- Portfolio value calculation (100 positions): 0.92ms vs 0.71ms (float) = **+29.6%**
- Returns calculation (252 days): 5.43ms vs 4.01ms = **+35.4%**
- Transaction processing (1000 txns): 234ms vs 182ms = **+28.6%**

### DecimalOrder
- Order submission (1000 orders): 187ms vs 145ms = **+29.0%**
- Order fill calculation: 0.43ms vs 0.34ms = **+26.5%**
- Commission calculation: 0.12ms vs 0.09ms = **+33.3%**

### Decimal Metrics
- Sharpe ratio (252 returns): 12.3ms vs 7.8ms = **+57.7%**  â† HOTSPOT
- Max drawdown (252 returns): 18.7ms vs 11.2ms = **+67.0%**  â† HOTSPOT
- VaR/CVaR (1000 returns): 23.4ms vs 14.1ms = **+66.0%**  â† HOTSPOT

### Data Pipeline
- Load 1 year daily bars: 245ms vs 198ms = **+23.7%**
- Parquet read overhead: 89ms vs 67ms = **+32.8%**
- OHLCV validation: 34ms vs 29ms = **+17.2%**

## Top 10 Hotspots (cProfile)

1. `Decimal.__add__` - 1.234s (18.7%)  â† Rust target
2. `calculate_max_drawdown` - 0.876s (13.3%)  â† Rust target
3. `Decimal.__mul__` - 0.654s (9.9%)  â† Rust target
4. `pl.Series.mean` - 0.543s (8.2%)
5. `calculate_sharpe_ratio` - 0.487s (7.4%)  â† Rust target
6. `DecimalLedger.portfolio_value` - 0.412s (6.3%)
7. `Decimal.__truediv__` - 0.389s (5.9%)  â† Rust target
8. `process_transaction` - 0.321s (4.9%)
9. `calculate_var` - 0.298s (4.5%)
10. `PolarsDataPortal.get_history_window` - 0.276s (4.2%)

## Optimization Targets for Epic 7

### Priority 1: Decimal Arithmetic (33.6% of runtime)
- Implement core Decimal operations in Rust: add, mul, div
- Use `rust-decimal` crate for high-performance Decimal
- Target: 50% reduction in arithmetic overhead

### Priority 2: Metrics Calculations (25.2% of runtime)
- Implement Sharpe, Sortino, drawdown in Rust
- Use SIMD for vectorized calculations
- Target: 60% reduction in metrics overhead

### Priority 3: Data Pipeline (8.3% of runtime)
- Optimize Parquet Decimal column loading
- Implement Rust-based type conversion
- Target: 40% reduction in data loading overhead

## Success Criteria for Epic 7

âœ… **Achieved:** <30% overall overhead (current: 28.3%)
ðŸŽ¯ **Target:** <12% overall overhead (aggressive but feasible with Rust)
ðŸŽ¯ **Stretch:** Match float performance (<5% overhead)

---

*Benchmark data: `benchmarks/results/decimal_backtest.json`*
```

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-01 | 1.0 | Initial story creation | Claude (Dev Agent) |

## Dev Agent Record

### Agent Model Used
(To be filled by Dev Agent)

### Debug Log References
(To be filled by Dev Agent)

### Completion Notes List
(To be filled by Dev Agent)

### File List
(To be filled by Dev Agent)

## QA Results
(To be filled by QA Agent)
