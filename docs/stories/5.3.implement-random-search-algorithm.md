# Story 5.3: Implement Random Search Algorithm

## Status
Draft

## Story
**As a** quantitative trader,
**I want** random sampling from parameter space,
**so that** I can efficiently explore high-dimensional spaces where grid search is impractical.

## Acceptance Criteria
1. RandomSearchAlgorithm implements SearchAlgorithm interface
2. Parameter distributions supported (uniform, log-uniform, normal, categorical)
3. Sample count configurable (e.g., 100 random samples)
4. Reproducible sampling (seed parameter for deterministic results)
5. Duplicate prevention (don't test same parameters twice)
6. Best result tracking during sampling
7. Parallel execution supported (distribute samples across workers)
8. Tests validate sampling distribution and duplicate prevention
9. Performance comparison vs. Grid Search demonstrated in documentation
10. Documentation explains when Random Search outperforms Grid Search (high dimensions)

## Tasks / Subtasks
- [ ] Implement RandomSearchAlgorithm class (AC: 1, 2, 3)
  - [ ] Create rustybt/optimization/search/random_search.py
  - [ ] Define RandomSearchAlgorithm inheriting from SearchAlgorithm base class
  - [ ] Implement parameter distribution specification using pydantic models
  - [ ] Support uniform distribution for continuous parameters
  - [ ] Support log-uniform distribution for scale-variant parameters
  - [ ] Support normal/gaussian distribution
  - [ ] Support categorical distribution for discrete choices
  - [ ] Implement sample count configuration (n_iter parameter)
  - [ ] Implement suggest() method generating random samples
  - [ ] Implement update() method recording results
  - [ ] Implement is_complete() method checking sample count reached
- [ ] Add reproducible sampling (AC: 4)
  - [ ] Add seed parameter to constructor
  - [ ] Use numpy.random.Generator with seed for reproducibility
  - [ ] Document seed usage for deterministic results
  - [ ] Test that same seed produces identical sample sequence
- [ ] Implement duplicate prevention (AC: 5)
  - [ ] Track previously generated parameter combinations
  - [ ] Check for duplicates before returning from suggest()
  - [ ] Re-sample if duplicate detected (up to max_retries)
  - [ ] Raise warning if duplicate rate exceeds threshold
- [ ] Implement best result tracking (AC: 6)
  - [ ] Track best parameters and objective value during sampling
  - [ ] Update best result on each update() call
  - [ ] Implement get_best_params() method
  - [ ] Implement get_best_result() method
- [ ] Add parallel execution support (AC: 7)
  - [ ] Make suggest() thread-safe with lock for sample counter
  - [ ] Make update() thread-safe for result aggregation
  - [ ] Test with multiprocessing to validate parallel safety
- [ ] Write comprehensive tests (AC: 8)
  - [ ] Test each distribution type (uniform, log-uniform, normal, categorical)
  - [ ] Test sample count enforcement (exactly n_iter samples)
  - [ ] Test reproducibility with seed
  - [ ] Test duplicate prevention
  - [ ] Test best result tracking
  - [ ] Test parallel execution correctness
  - [ ] Property test: sample count equals n_iter
  - [ ] Property test: samples within distribution bounds
- [ ] Create performance comparison (AC: 9)
  - [ ] Create benchmark comparing Random vs. Grid Search
  - [ ] Test on 5-parameter space (Random 100 samples vs. Grid 3^5=243)
  - [ ] Measure time to find good parameters (within 95% of optimal)
  - [ ] Document results in docs/architecture/optimization.md
- [ ] Create example notebook (AC: 9)
  - [ ] Create examples/optimization/random_search_comparison.ipynb
  - [ ] Demonstrate Random Search on 5-parameter strategy
  - [ ] Compare performance vs. Grid Search (time and quality)
  - [ ] Visualize distribution coverage across parameter space
  - [ ] Show efficiency gains for high-dimensional spaces
  - [ ] Demonstrate different distribution types (uniform, log-uniform, normal)
- [ ] Add documentation (AC: 10)
  - [ ] Document when Random Search outperforms Grid Search
  - [ ] Explain curse of dimensionality for grid search
  - [ ] Show Random Search finds good params with fewer evaluations in high dimensions
  - [ ] Add usage examples with different distributions
  - [ ] Document recommended sample counts (10-100× parameter count)

## Dev Notes

### Previous Story Context
[Source: Story 5.1, Story 5.2]
- Optimization framework architecture defined in Story 5.1
- SearchAlgorithm interface implemented by GridSearchAlgorithm in Story 5.2
- ParameterSpace and result storage patterns established

### Relevant Source Tree Info
[Source: [source-tree.md](docs/architecture/source-tree.md#L106-L117)]
File location: `rustybt/optimization/search/random_search.py`

Test location: `tests/optimization/search/test_random_search.py`

Example location: `examples/optimization/random_search_comparison.ipynb`

### SearchAlgorithm Interface
[Source: Story 5.1, AC 2]
RandomSearchAlgorithm must implement:
- `suggest() -> Dict[str, Any]`: Return random parameter sample
- `update(params: Dict[str, Any], result: float) -> None`: Record result
- `is_complete() -> bool`: Return True when n_iter samples completed

### Parameter Distribution Types
[Source: AC 2]

**Uniform Distribution** (continuous parameters):
```python
{
    'type': 'uniform',
    'low': 10.0,
    'high': 100.0
}
```

**Log-Uniform Distribution** (scale-variant parameters like learning rate):
```python
{
    'type': 'log-uniform',
    'low': 1e-4,
    'high': 1e-1
}
```

**Normal Distribution**:
```python
{
    'type': 'normal',
    'mean': 50.0,
    'std': 10.0
}
```

**Categorical Distribution** (discrete choices):
```python
{
    'type': 'categorical',
    'choices': ['ema', 'sma', 'wma']
}
```

### Reproducible Sampling
[Source: AC 4]
```python
import numpy as np

rng = np.random.Generator(np.random.PCG64(seed=42))
sample = rng.uniform(low, high)
```

Use numpy.random.Generator (not legacy np.random) for:
- Better statistical properties
- Thread-safe random number generation
- Reproducible results with seed

### Duplicate Prevention Strategy
[Source: AC 5]
- Store parameter combinations in set (use tuple of sorted items for hashability)
- Check set before returning sample
- If duplicate, re-sample up to max_retries (default: 100)
- If max_retries exceeded, log warning but allow duplicate (prevents infinite loop)

### Sample Count Configuration
[Source: AC 3]
Recommended heuristics:
- Small spaces (<5 params): n_iter = 50-100
- Medium spaces (5-10 params): n_iter = 100-500
- Large spaces (>10 params): n_iter = 500-1000
- Rule of thumb: 10-100× number of parameters

### Parallel Execution
[Source: AC 7]
Thread-safety requirements:
- Lock on sample_counter (threading.Lock)
- Lock on results list for update()
- Lock on seen_params set for duplicate check

### Checkpoint/Resume Support
[Source: Story 5.1, AC 6]
**Implementation Decision**: Checkpoint/resume support designed in Story 5.1 will be implemented in the base Optimizer class that wraps SearchAlgorithm instances, not in individual algorithm implementations.

RandomSearchAlgorithm should maintain serializable state:
- Sample counter (how many samples generated)
- Random generator state (for reproducible resume)
- Seen parameters set (for duplicate prevention)
- Results collected so far
- Best parameters found

The Optimizer wrapper (from Story 5.1) will handle saving/loading this state to enable resume.

### Performance Comparison
[Source: AC 9]

Research finding (Bergstra & Bengio, 2012):
> "Random search is more efficient than grid search for hyperparameter optimization when only a small number of hyperparameters effectively influence the final performance."

Demonstrate:
- 5D space: Random (100 samples) finds 95% optimal in 100 evals
- Grid search needs 3^5 = 243 evals for same coverage
- Time savings: 2.4× faster

### When Random Search Outperforms Grid Search
[Source: AC 10]

**Use Random Search when:**
1. High-dimensional spaces (>5 parameters)
2. Some parameters less important than others
3. Continuous parameters (infinite grid points)
4. Limited computational budget
5. Quick exploration phase before Bayesian optimization

**Use Grid Search when:**
1. Low-dimensional spaces (<5 parameters)
2. All parameters equally important
3. Small discrete parameter sets
4. Exhaustive search required for compliance/validation

### Tech Stack
[Source: [tech-stack.md](docs/architecture/tech-stack.md)]
- numpy for random sampling
- pydantic for distribution config validation
- scipy.stats for advanced distributions (if needed)

### Testing

#### Test File Location
[Source: [testing-strategy.md](docs/architecture/testing-strategy.md#L441-L444)]
Tests at: `tests/optimization/search/test_random_search.py`

#### Coverage Requirements
[Source: [testing-strategy.md](docs/architecture/testing-strategy.md#L7)]
- **New Components**: ≥90% strict enforcement

#### Distribution Validation Tests
[Source: AC 8]
```python
def test_uniform_distribution_bounds():
    """Samples must be within [low, high]."""
    random_search = RandomSearchAlgorithm(
        param_distributions={'x': {'type': 'uniform', 'low': 0, 'high': 1}},
        n_iter=1000,
        seed=42
    )
    samples = [random_search.suggest()['x'] for _ in range(1000)]
    assert all(0 <= s <= 1 for s in samples)
    # Check distribution is approximately uniform
    assert 0.4 < np.mean(samples) < 0.6  # Mean should be near 0.5
```

#### Reproducibility Test
[Source: AC 4, AC 8]
```python
def test_seed_reproducibility():
    """Same seed produces identical sample sequence."""
    rs1 = RandomSearchAlgorithm(param_distributions={...}, n_iter=10, seed=42)
    rs2 = RandomSearchAlgorithm(param_distributions={...}, n_iter=10, seed=42)

    samples1 = [rs1.suggest() for _ in range(10)]
    samples2 = [rs2.suggest() for _ in range(10)]

    assert samples1 == samples2
```

#### Property Tests
[Source: [testing-strategy.md](docs/architecture/testing-strategy.md#L26-L85)]
```python
@given(
    n_iter=st.integers(min_value=10, max_value=100)
)
def test_sample_count_invariant(n_iter):
    """Random search must generate exactly n_iter samples."""
    rs = RandomSearchAlgorithm(param_distributions={...}, n_iter=n_iter)
    samples = []
    while not rs.is_complete():
        samples.append(rs.suggest())
    assert len(samples) == n_iter
```

#### Zero-Mock Enforcement
[Source: [coding-standards.md](docs/architecture/coding-standards.md#L137-L212)]
- No mocked random number generators
- Tests use real numpy.random.Generator
- Validation tests check actual distribution properties (mean, std, bounds)

### Documentation

#### Docstring Example
[Source: [coding-standards.md](docs/architecture/coding-standards.md#L48-L74)]
```python
class RandomSearchAlgorithm(SearchAlgorithm):
    """Random search optimization algorithm.

    Samples parameter combinations randomly from specified distributions.
    More efficient than grid search for high-dimensional spaces (>5 params).

    Recommended for:
        - Initial exploration of parameter space
        - High-dimensional optimization (>5 parameters)
        - Limited computational budget
        - Continuous parameter ranges

    Args:
        param_distributions: Dict mapping param names to distribution configs
        n_iter: Number of random samples to generate
        seed: Random seed for reproducibility (optional)

    Example:
        >>> random_search = RandomSearchAlgorithm(
        ...     param_distributions={
        ...         'lookback': {'type': 'uniform', 'low': 10, 'high': 100},
        ...         'threshold': {'type': 'log-uniform', 'low': 1e-3, 'high': 1e-1}
        ...     },
        ...     n_iter=100,
        ...     seed=42
        ... )
        >>> while not random_search.is_complete():
        ...     params = random_search.suggest()
        ...     result = run_backtest(**params)
        ...     random_search.update(params, result['sharpe_ratio'])
        >>> best_params = random_search.get_best_params()
    """
```

#### Architecture Documentation
[Source: AC 10]
Add to docs/architecture/optimization.md:
- Comparison table: Random vs. Grid Search
- When to use each algorithm
- Performance benchmarks
- Distribution selection guide

### Type Hints and Validation
[Source: [coding-standards.md](docs/architecture/coding-standards.md#L9-L25)]
- 100% type hint coverage
- pydantic models for distribution configs
- mypy --strict compliance

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-02 | 1.0 | Initial story creation | Bob (SM Agent) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent_

### Debug Log References
_To be populated by dev agent_

### Completion Notes List
_To be populated by dev agent_

### File List
_To be populated by dev agent_

## QA Results
_To be populated by QA agent_
