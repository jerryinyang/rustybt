# Story 5.5: Implement Genetic Algorithm Optimization

## Status
Draft

## Story
**As a** quantitative trader,
**I want** genetic algorithm optimization inspired by natural selection,
**so that** I can explore complex parameter landscapes with crossover and mutation operators.

## Acceptance Criteria
1. GeneticAlgorithm implements SearchAlgorithm interface using DEAP library
2. Population size configurable (e.g., 50 individuals)
3. Selection operator configurable (tournament, roulette, rank-based)
4. Crossover operator implemented (combine parameters from two parents)
5. Mutation operator implemented (randomly perturb parameters)
6. Elite preservation (keep top N individuals across generations)
7. Termination criteria (max generations, fitness plateau, or time limit)
8. Population diversity tracking (prevent premature convergence)
9. Tests validate GA finds good solutions and population evolves over generations
10. Example demonstrates GA on non-smooth objective function (where Bayesian struggles)

## Tasks / Subtasks
- [ ] Implement GeneticAlgorithm class (AC: 1, 2, 7)
  - [ ] Create rustybt/optimization/search/genetic_algorithm.py
  - [ ] Install and import DEAP library
  - [ ] Define GeneticAlgorithm inheriting from SearchAlgorithm base class
  - [ ] Implement parameter space encoding (genotype representation)
  - [ ] Initialize population with random individuals
  - [ ] Configure population size (default: 50)
  - [ ] Implement generation loop with termination criteria
  - [ ] Track generation count, best fitness, avg fitness
- [ ] Implement selection operators (AC: 3)
  - [ ] Tournament selection (select best from random subset)
  - [ ] Roulette wheel selection (fitness-proportional)
  - [ ] Rank-based selection (based on fitness rank, not value)
  - [ ] Make selection operator configurable
  - [ ] Document when to use each selection method
- [ ] Implement crossover operator (AC: 4)
  - [ ] Single-point crossover for continuous parameters
  - [ ] Uniform crossover (50% chance each gene from each parent)
  - [ ] Blend crossover (BLX-α) for continuous parameters
  - [ ] Respect parameter bounds after crossover
  - [ ] Make crossover probability configurable (default: 0.8)
- [ ] Implement mutation operator (AC: 5)
  - [ ] Gaussian mutation for continuous parameters
  - [ ] Uniform mutation (random reset)
  - [ ] Polynomial mutation (DEAP default)
  - [ ] Respect parameter bounds after mutation
  - [ ] Make mutation probability configurable (default: 0.2)
  - [ ] Adaptive mutation rate (decrease over generations optional)
- [ ] Implement elitism (AC: 6)
  - [ ] Preserve top N individuals (elite_size parameter)
  - [ ] Copy elites to next generation unchanged
  - [ ] Default elite_size = 10% of population
  - [ ] Document elitism prevents losing best solutions
- [ ] Add termination criteria (AC: 7)
  - [ ] Max generations (default: 100)
  - [ ] Fitness plateau (no improvement for N generations)
  - [ ] Time limit (max_time_seconds optional)
  - [ ] Fitness threshold (stop if fitness >= target)
  - [ ] Log termination reason
- [ ] Implement diversity tracking (AC: 8)
  - [ ] Calculate population diversity (std of gene values)
  - [ ] Track diversity over generations
  - [ ] Warn if diversity drops below threshold (premature convergence)
  - [ ] Optional diversity-preserving mechanisms (niching, crowding)
- [ ] Write comprehensive tests (AC: 9)
  - [ ] Test each selection operator
  - [ ] Test crossover preserves bounds and mixes genes
  - [ ] Test mutation adds variation
  - [ ] Test elitism preserves best individuals
  - [ ] Test fitness improves over generations
  - [ ] Test diversity tracking
  - [ ] Test termination criteria
  - [ ] Property test: all individuals respect parameter bounds
- [ ] Create example notebook (AC: 10)
  - [ ] Create examples/optimization/genetic_algorithm_nonsmooth.ipynb
  - [ ] Demonstrate GA on non-smooth Rastrigin function (many local optima)
  - [ ] Compare with Bayesian optimization (show GA performs better)
  - [ ] Visualize population evolution over generations
  - [ ] Show diversity tracking preventing premature convergence
- [ ] Add documentation
  - [ ] Explain genetic algorithm intuition (natural selection)
  - [ ] Document operator selection guide (selection, crossover, mutation)
  - [ ] Document when GA outperforms Bayesian (non-smooth, multimodal)
  - [ ] Add usage examples with different configurations

## Dev Notes

### Previous Story Context
[Source: Story 5.1, Story 5.2, Story 5.3, Story 5.4]
- Optimization framework architecture defined in Story 5.1
- SearchAlgorithm interface implemented by Grid, Random, Bayesian
- Bayesian optimization works well for smooth functions
- Genetic algorithms excel on non-smooth, multimodal landscapes

### Relevant Source Tree Info
[Source: [source-tree.md](docs/architecture/source-tree.md#L106-L117)]
File location: `rustybt/optimization/search/genetic_algorithm.py`

Test location: `tests/optimization/search/test_genetic_algorithm.py`

Example location: `examples/optimization/genetic_algorithm_nonsmooth.ipynb`

### SearchAlgorithm Interface
[Source: Story 5.1, AC 2]
GeneticAlgorithm must implement:
- `suggest() -> Dict[str, Any]`: Return individual for evaluation (from current generation)
- `update(params: Dict[str, Any], result: float) -> None`: Record fitness for individual
- `is_complete() -> bool`: Return True when termination criteria met

### DEAP Library
[Source: AC 1]
DEAP (Distributed Evolutionary Algorithms in Python):
- Industry-standard GA library
- Provides toolbox for selection, crossover, mutation
- Supports custom fitness functions and constraints

```python
from deap import base, creator, tools, algorithms
```

### Genotype Representation
Encode parameters as list of floats (genotype):
```python
# Parameter dict
params = {'lookback': 20, 'threshold': 0.02, 'rebalance': 5}

# Genotype encoding
individual = [20.0, 0.02, 5.0]
```

Decode back to parameter dict for evaluation.

### Population Initialization
[Source: AC 2]
```python
from deap import creator, base, tools

creator.create("FitnessMax", base.Fitness, weights=(1.0,))  # Maximize
creator.create("Individual", list, fitness=creator.FitnessMax)

toolbox = base.Toolbox()
toolbox.register("attr_float", random.uniform, low, high)
toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_float, n=n_params)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

population = toolbox.population(n=50)
```

### Selection Operators
[Source: AC 3]

**Tournament Selection** (default, robust):
```python
toolbox.register("select", tools.selTournament, tournsize=3)
```
Randomly select K individuals, pick best. Balances diversity and selection pressure.

**Roulette Wheel Selection** (fitness-proportional):
```python
toolbox.register("select", tools.selRoulette)
```
Probability ∝ fitness. Can lead to premature convergence.

**Rank-Based Selection**:
```python
toolbox.register("select", tools.selRankLinear, s=2.0)
```
Based on fitness rank, not magnitude. Robust to outliers.

### Crossover Operators
[Source: AC 4]

**Blend Crossover (BLX-α)** - Recommended for continuous:
```python
toolbox.register("mate", tools.cxBlend, alpha=0.5)
```

**Uniform Crossover**:
```python
toolbox.register("mate", tools.cxUniform, indpb=0.5)
```

Crossover probability: 0.7-0.9 (default: 0.8)

### Mutation Operators
[Source: AC 5]

**Gaussian Mutation** - Recommended for continuous:
```python
toolbox.register("mutate", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.2)
```

**Polynomial Mutation**:
```python
toolbox.register("mutate", tools.mutPolynomialBounded, low=low, up=high, eta=20.0, indpb=0.2)
```

Mutation probability: 0.1-0.3 (default: 0.2)

### Elitism
[Source: AC 6]
```python
# Select elites (top 10%)
elite_size = int(0.1 * population_size)
elites = tools.selBest(population, elite_size)

# Create offspring via selection/crossover/mutation
offspring = algorithms.varAnd(population, toolbox, cxpb=0.8, mutpb=0.2)

# Replace population, preserving elites
population[:] = elites + tools.selBest(offspring, population_size - elite_size)
```

### Termination Criteria
[Source: AC 7]

Stop when:
1. `max_generations` reached, OR
2. No fitness improvement for `patience` generations, OR
3. Time limit `max_time_seconds` exceeded, OR
4. Fitness >= `target_fitness` (if specified)

### Diversity Tracking
[Source: AC 8]
```python
def population_diversity(population, n_params):
    """Calculate population diversity as avg std dev across genes."""
    genes = np.array([ind[:] for ind in population])  # (pop_size, n_params)
    diversity = np.mean([np.std(genes[:, i]) for i in range(n_params)])
    return diversity

# Warn if diversity < threshold
if diversity < 0.01 * (param_max - param_min):
    logger.warning("Low diversity detected - premature convergence risk")
```

### When GA Outperforms Bayesian
[Source: AC 10]

**Use Genetic Algorithm when:**
1. **Non-smooth objective** (discontinuities, noise)
2. **Multimodal landscape** (many local optima)
3. **Discrete/mixed parameters** (GA handles categorical better)
4. **Large populations needed** (explore diverse solutions)
5. **Cheap evaluations** (GA needs many evaluations)

**Don't use when:**
1. Smooth, unimodal objective → Bayesian faster
2. Expensive evaluations → Bayesian more sample-efficient
3. Very high dimensions (>50 params) → Curse of dimensionality

### Test Function: Rastrigin
[Source: AC 10]
Non-smooth, multimodal function (GA excels):
```python
def rastrigin(x):
    """Rastrigin function with many local minima."""
    A = 10
    n = len(x)
    return -(A * n + sum(x_i**2 - A * np.cos(2 * np.pi * x_i) for x_i in x))
```

Bayesian struggles due to many local optima, GA performs better.

### Checkpoint/Resume Support
[Source: Story 5.1, AC 6]
**Implementation Decision**: Checkpoint/resume support designed in Story 5.1 will be implemented in the base Optimizer class that wraps SearchAlgorithm instances, not in individual algorithm implementations.

GeneticAlgorithm should maintain serializable state:
- Current population (all individuals)
- Generation counter
- Best individuals (elites)
- Fitness history across generations
- Diversity metrics history
- Random state for reproducibility

The Optimizer wrapper (from Story 5.1) will handle saving/loading this state. DEAP population and toolbox can be pickled for seamless resume.

### Tech Stack
[Source: [tech-stack.md](docs/architecture/tech-stack.md)]
- **DEAP**: Genetic algorithm framework
- numpy for numerical operations
- matplotlib for population evolution visualization

### Dependency Installation
Add to pyproject.toml:
```toml
[project.optional-dependencies]
optimization = [
    "deap>=1.4.0",
]
```

### Testing

#### Test File Location
[Source: [testing-strategy.md](docs/architecture/testing-strategy.md#L441-L444)]
Tests at: `tests/optimization/search/test_genetic_algorithm.py`

#### Coverage Requirements
[Source: [testing-strategy.md](docs/architecture/testing-strategy.md#L7)]
- **New Components**: ≥90% strict enforcement

#### Evolution Test
[Source: AC 9]
```python
def test_fitness_improves_over_generations():
    """Population fitness should improve over generations."""
    ga = GeneticAlgorithm(
        param_bounds={'x': (-5, 5), 'y': (-5, 5)},
        population_size=50,
        max_generations=20
    )

    def sphere(params):
        """Simple sphere function (unimodal)."""
        return -(params['x']**2 + params['y']**2)

    generation_best = []
    while not ga.is_complete():
        params = ga.suggest()
        fitness = sphere(params)
        ga.update(params, fitness)
        if ga.generation_complete():
            generation_best.append(ga.get_best_result())

    # Fitness should improve (or stay same) over generations
    assert generation_best[-1] >= generation_best[0]
    # Should find near-optimal solution
    assert generation_best[-1] >= -0.1  # Close to optimum (0)
```

#### Property Tests
[Source: [testing-strategy.md](docs/architecture/testing-strategy.md#L26-L85)]
```python
@given(
    pop_size=st.integers(min_value=10, max_value=100),
    n_params=st.integers(min_value=2, max_value=5)
)
def test_all_individuals_respect_bounds(pop_size, n_params):
    """All individuals must be within parameter bounds."""
    bounds = {f'x{i}': (0, 1) for i in range(n_params)}
    ga = GeneticAlgorithm(param_bounds=bounds, population_size=pop_size)

    for _ in range(pop_size):
        params = ga.suggest()
        # All parameters in [0, 1]
        assert all(0 <= params[f'x{i}'] <= 1 for i in range(n_params))
```

#### Zero-Mock Enforcement
[Source: [coding-standards.md](docs/architecture/coding-standards.md#L137-L212)]
- No mocked selection/crossover/mutation operators
- Tests use real DEAP toolbox
- Evolution tests run actual GA on test functions

### Documentation

#### Docstring Example
[Source: [coding-standards.md](docs/architecture/coding-standards.md#L48-L74)]
```python
class GeneticAlgorithm(SearchAlgorithm):
    """Genetic algorithm optimization using natural selection principles.

    Evolves population of candidate solutions using selection, crossover, and
    mutation operators. Excels on non-smooth, multimodal objective functions.

    Best for:
        - Non-smooth or discontinuous objectives
        - Multimodal landscapes (many local optima)
        - Mixed parameter types (continuous + categorical)
        - Cheap evaluations (GA needs 100s-1000s of evaluations)

    Args:
        param_bounds: Dict mapping param names to (min, max) tuples
        population_size: Number of individuals in population (default: 50)
        max_generations: Maximum generations (default: 100)
        selection: Selection operator ('tournament', 'roulette', 'rank')
        crossover_prob: Crossover probability (default: 0.8)
        mutation_prob: Mutation probability (default: 0.2)
        elite_size: Number of elites to preserve (default: 10% of pop)

    Example:
        >>> ga = GeneticAlgorithm(
        ...     param_bounds={'lookback': (10, 100), 'threshold': (0.01, 0.1)},
        ...     population_size=50,
        ...     max_generations=100,
        ...     selection='tournament'
        ... )
        >>> while not ga.is_complete():
        ...     params = ga.suggest()
        ...     result = run_backtest(**params)
        ...     ga.update(params, result['sharpe_ratio'])
        >>> best_params = ga.get_best_params()
    """
```

#### Architecture Documentation
Add to docs/architecture/optimization.md:
- Genetic algorithm principles (selection, crossover, mutation, elitism)
- Operator selection guide
- When GA outperforms Bayesian (non-smooth functions)
- Diversity preservation importance

### Type Hints and Validation
[Source: [coding-standards.md](docs/architecture/coding-standards.md#L9-L25)]
- 100% type hint coverage
- pydantic models for config validation
- mypy --strict compliance

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-02 | 1.0 | Initial story creation | Bob (SM Agent) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent_

### Debug Log References
_To be populated by dev agent_

### Completion Notes List
_To be populated by dev agent_

### File List
_To be populated by dev agent_

## QA Results
_To be populated by QA agent_
