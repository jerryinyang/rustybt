# Story 5.6: Implement Parallel Processing Framework

## Status
Draft

## Story
**As a** quantitative trader,
**I want** parallel optimization execution across multiple cores/machines,
**so that** I can achieve significant speedup for optimization campaigns.

## Acceptance Criteria
1. ParallelOptimizer wraps SearchAlgorithm with parallel execution
2. Local parallelization using multiprocessing (utilize all CPU cores)
3. Distributed parallelization using Ray (scale across multiple machines optional)
4. Worker pool management (spawn, monitor, restart failed workers)
5. Task queue management (distribute parameter evaluations to workers)
6. Result aggregation from parallel workers (thread-safe result collection)
7. Progress monitoring (live updates of optimization progress across workers)
8. Resource limits configurable (max CPUs, max memory per worker)
9. Tests validate parallel execution produces identical results to serial (deterministic)
10. Benchmark demonstrates near-linear speedup up to available cores for typical optimization

## Tasks / Subtasks
- [ ] Implement ParallelOptimizer class (AC: 1, 2)
  - [ ] Create rustybt/optimization/parallel_optimizer.py
  - [ ] Define ParallelOptimizer wrapping any SearchAlgorithm
  - [ ] Implement local parallelization using multiprocessing.Pool
  - [ ] Support configurable number of workers (default: cpu_count())
  - [ ] Integrate with Grid, Random, Bayesian, Genetic algorithms
  - [ ] Preserve SearchAlgorithm interface (suggest/update/is_complete)
- [ ] Implement worker pool management (AC: 4)
  - [ ] Spawn worker processes with multiprocessing.Pool
  - [ ] Monitor worker health (detect crashes, hangs)
  - [ ] Restart failed workers automatically
  - [ ] Graceful shutdown on completion or error
  - [ ] Implement timeout for individual evaluations
- [ ] Implement task queue (AC: 5)
  - [ ] Create task queue for parameter evaluations
  - [ ] Distribute tasks to available workers
  - [ ] Handle task failures (retry or skip)
  - [ ] Load balancing (dynamic task assignment)
  - [ ] Support both batch and streaming task submission
- [ ] Implement thread-safe result aggregation (AC: 6)
  - [ ] Use multiprocessing.Queue for result collection
  - [ ] Thread-safe update to SearchAlgorithm state
  - [ ] Preserve result ordering if required
  - [ ] Handle partial results from failed workers
- [ ] Add progress monitoring (AC: 7)
  - [ ] Track completed/pending/failed evaluations
  - [ ] Real-time progress updates (progress bar or logging)
  - [ ] Per-worker statistics (throughput, failures)
  - [ ] Overall optimization status (best result, remaining time estimate)
- [ ] Implement resource limits (AC: 8)
  - [ ] Max CPUs configurable (n_jobs parameter)
  - [ ] Max memory per worker (process limits)
  - [ ] CPU affinity for worker processes (optional)
  - [ ] Prevent resource exhaustion
- [ ] Add distributed parallelization with Ray (AC: 3)
  - [ ] Create RayParallelOptimizer variant (optional)
  - [ ] Use Ray tasks for distributed execution
  - [ ] Support multi-machine scaling
  - [ ] Handle Ray cluster setup and teardown
  - [ ] Document Ray setup and configuration
- [ ] Write comprehensive tests (AC: 9)
  - [ ] Test determinism (serial vs parallel produce same best result)
  - [ ] Test worker failure handling
  - [ ] Test task queue correctness
  - [ ] Test result aggregation with concurrent updates
  - [ ] Test resource limit enforcement
  - [ ] Test progress monitoring accuracy
- [ ] Create performance benchmarks (AC: 10)
  - [ ] Benchmark on standard optimization problem
  - [ ] Measure speedup vs. serial (1, 2, 4, 8 workers)
  - [ ] Plot speedup curve (should be near-linear up to core count)
  - [ ] Document expected speedup and overhead
  - [ ] Identify bottlenecks (if any)
- [ ] Add documentation
  - [ ] Document when to use parallel optimization
  - [ ] Explain parallelization overhead (synchronization, communication)
  - [ ] Document Ray setup for distributed optimization
  - [ ] Add usage examples with different algorithms
  - [ ] Document performance best practices

## Dev Notes

### Previous Story Context
[Source: Story 5.1, Story 5.2, Story 5.3, Story 5.4, Story 5.5]
- Optimization framework with Grid, Random, Bayesian, Genetic algorithms
- Each algorithm implements SearchAlgorithm interface (suggest/update/is_complete)
- Optimization campaigns run 100s-1000s of backtests (CPU-intensive)
- Parallelization critical for reasonable completion times

### Relevant Source Tree Info
[Source: [source-tree.md](docs/architecture/source-tree.md#L106-L117)]
File location: `rustybt/optimization/parallel_optimizer.py`

Test location: `tests/optimization/test_parallel_optimizer.py`

### SearchAlgorithm Interface
[Source: Story 5.1, AC 2]
ParallelOptimizer must wrap any SearchAlgorithm:
- `suggest() -> Dict[str, Any]`: Thread-safe parameter suggestion
- `update(params: Dict[str, Any], result: float) -> None`: Thread-safe result update
- `is_complete() -> bool`: Check completion status

### Parallelization Strategy
[Source: AC 2]

**Multiprocessing Pool** (local):
```python
from multiprocessing import Pool, cpu_count

def evaluate_params(params):
    """Worker function: run backtest and return result."""
    result = run_backtest(**params)
    return params, result['sharpe_ratio']

with Pool(processes=cpu_count()) as pool:
    # Get batch of parameters
    param_batch = [optimizer.suggest() for _ in range(batch_size)]
    # Evaluate in parallel
    results = pool.map(evaluate_params, param_batch)
    # Update optimizer with results
    for params, result in results:
        optimizer.update(params, result)
```

### Worker Pool Management
[Source: AC 4]

**Spawn workers**:
```python
pool = multiprocessing.Pool(
    processes=n_jobs,
    initializer=worker_init,
    maxtasksperchild=100  # Restart workers periodically
)
```

**Monitor health**:
- Detect worker crashes via pool callbacks
- Timeout individual tasks (max_eval_time)
- Restart failed workers automatically

### Task Queue Management
[Source: AC 5]

**Dynamic task submission**:
```python
# Stream tasks as workers become available
async_results = []
while not optimizer.is_complete():
    if len(async_results) < n_jobs * 2:  # Keep queue full
        params = optimizer.suggest()
        async_result = pool.apply_async(evaluate_params, (params,))
        async_results.append((params, async_result))

    # Collect completed results
    for params, ar in async_results[:]:
        if ar.ready():
            result = ar.get(timeout=1)
            optimizer.update(params, result)
            async_results.remove((params, ar))
```

### Thread-Safe Result Aggregation
[Source: AC 6]

**Use multiprocessing.Queue**:
```python
from multiprocessing import Queue

result_queue = Queue()

def worker(params):
    result = run_backtest(**params)
    result_queue.put((params, result))

# Main thread collects results
while not optimizer.is_complete():
    try:
        params, result = result_queue.get(timeout=1)
        optimizer.update(params, result)  # Thread-safe update
    except queue.Empty:
        continue
```

**Lock for optimizer state**:
```python
from threading import Lock

optimizer_lock = Lock()

def update_optimizer(params, result):
    with optimizer_lock:
        optimizer.update(params, result)
```

### Progress Monitoring
[Source: AC 7]

**Progress tracking**:
```python
from tqdm import tqdm

with tqdm(total=optimizer.total_iterations, desc="Optimization") as pbar:
    while not optimizer.is_complete():
        # Submit tasks...
        # Collect results...
        pbar.update(1)
        pbar.set_postfix({
            'best': optimizer.get_best_result(),
            'workers': n_active_workers
        })
```

**Per-worker stats**:
- Throughput (evals/sec)
- Success/failure rate
- Average evaluation time

### Resource Limits
[Source: AC 8]

**CPU limit**:
```python
n_jobs = min(requested_workers, cpu_count())
```

**Memory limit** (per worker):
```python
import resource

def limit_memory():
    """Limit worker memory to 4GB."""
    resource.setrlimit(resource.RLIMIT_AS, (4 * 1024**3, 4 * 1024**3))

pool = Pool(processes=n_jobs, initializer=limit_memory)
```

**CPU affinity** (optional):
```python
import psutil

def set_affinity(cpu_id):
    p = psutil.Process()
    p.cpu_affinity([cpu_id])
```

### Distributed Parallelization with Ray
[Source: AC 3]

**Ray setup**:
```python
import ray

ray.init(address='auto')  # Connect to Ray cluster

@ray.remote
def evaluate_params_ray(params):
    """Ray remote function."""
    result = run_backtest(**params)
    return params, result['sharpe_ratio']

# Submit tasks
futures = [evaluate_params_ray.remote(params) for params in param_batch]
results = ray.get(futures)
```

**Multi-machine scaling**:
- Deploy Ray cluster on multiple nodes
- ParallelOptimizer submits tasks to cluster
- Automatic load balancing across nodes

### Determinism
[Source: AC 9]

**Challenge**: Parallel execution order is non-deterministic.

**Solution**: Best result should be identical (or very close) to serial:
- Use same random seed for SearchAlgorithm
- Ensure optimizer state updates are commutative
- Test: serial best == parallel best (within tolerance)

### Performance Benchmark
[Source: AC 10]

**Expected speedup**:
- 1 worker: 1.0× (baseline)
- 2 workers: ~1.9× (95% efficiency)
- 4 workers: ~3.7× (92% efficiency)
- 8 workers: ~6.5× (81% efficiency)

**Overhead sources**:
- Inter-process communication
- Result aggregation synchronization
- Task queue management

**Amdahl's Law**:
```
Speedup = 1 / ((1 - P) + P/N)
where P = parallelizable fraction, N = workers
```

For CPU-bound backtests (P ≈ 0.95), expect near-linear speedup.

### Tech Stack
[Source: [tech-stack.md](docs/architecture/tech-stack.md)]
- **multiprocessing**: Local parallelization
- **Ray** (optional): Distributed parallelization
- tqdm for progress bars
- psutil for resource monitoring

### Dependency Installation
Add to pyproject.toml:
```toml
[project.optional-dependencies]
optimization = [
    "ray[default]>=2.0.0",  # Optional for distributed
    "tqdm>=4.0.0",
    "psutil>=5.0.0",
]
```

### Testing

#### Test File Location
[Source: [testing-strategy.md](docs/architecture/testing-strategy.md#L441-L444)]
Tests at: `tests/optimization/test_parallel_optimizer.py`

#### Coverage Requirements
[Source: [testing-strategy.md](docs/architecture/testing-strategy.md#L7)]
- **New Components**: ≥90% strict enforcement

#### Determinism Test
[Source: AC 9]
```python
def test_parallel_determinism():
    """Parallel should find same best result as serial."""

    def sphere(params):
        return -(params['x']**2 + params['y']**2)

    # Serial optimization
    serial_opt = RandomSearchAlgorithm(
        param_distributions={'x': {'type': 'uniform', 'low': -5, 'high': 5},
                             'y': {'type': 'uniform', 'low': -5, 'high': 5}},
        n_iter=100,
        seed=42
    )
    while not serial_opt.is_complete():
        params = serial_opt.suggest()
        serial_opt.update(params, sphere(params))
    serial_best = serial_opt.get_best_result()

    # Parallel optimization
    parallel_opt = ParallelOptimizer(
        algorithm=RandomSearchAlgorithm(..., seed=42),
        n_jobs=4
    )
    parallel_opt.run(sphere)
    parallel_best = parallel_opt.get_best_result()

    # Results should be identical (same seed, deterministic)
    assert abs(serial_best - parallel_best) < 1e-6
```

#### Speedup Benchmark
[Source: AC 10]
```python
def test_parallel_speedup():
    """Parallel should achieve near-linear speedup."""
    import time

    def slow_objective(params):
        time.sleep(0.1)  # Simulate slow backtest
        return -(params['x']**2)

    # Serial baseline
    start = time.time()
    serial_opt = GridSearchAlgorithm(param_grid={'x': list(range(20))})
    while not serial_opt.is_complete():
        params = serial_opt.suggest()
        serial_opt.update(params, slow_objective(params))
    serial_time = time.time() - start

    # Parallel (4 workers)
    start = time.time()
    parallel_opt = ParallelOptimizer(
        algorithm=GridSearchAlgorithm(param_grid={'x': list(range(20))}),
        n_jobs=4
    )
    parallel_opt.run(slow_objective)
    parallel_time = time.time() - start

    speedup = serial_time / parallel_time
    # Expect ~3.5× speedup with 4 workers (allow 10% overhead)
    assert speedup >= 3.0
```

#### Zero-Mock Enforcement
[Source: [coding-standards.md](docs/architecture/coding-standards.md#L137-L212)]
- No mocked multiprocessing.Pool
- Tests use real parallel execution
- Benchmarks measure actual speedup on real workloads

### Documentation

#### Docstring Example
[Source: [coding-standards.md](docs/architecture/coding-standards.md#L48-L74)]
```python
class ParallelOptimizer:
    """Parallel optimization wrapper for SearchAlgorithm instances.

    Distributes parameter evaluations across multiple CPU cores (or machines
    with Ray) to achieve significant speedup for optimization campaigns.

    Best for:
        - CPU-bound objective functions (backtests)
        - Large optimization campaigns (100+ evaluations)
        - Multi-core machines (4+ cores)

    Args:
        algorithm: SearchAlgorithm instance (Grid, Random, Bayesian, Genetic)
        n_jobs: Number of parallel workers (default: cpu_count())
        backend: 'multiprocessing' or 'ray' (default: 'multiprocessing')
        max_eval_time: Timeout per evaluation in seconds (optional)
        verbose: Show progress bar (default: True)

    Example:
        >>> from rustybt.optimization import RandomSearchAlgorithm, ParallelOptimizer
        >>> algorithm = RandomSearchAlgorithm(
        ...     param_distributions={'lookback': {'type': 'uniform', 'low': 10, 'high': 100}},
        ...     n_iter=100
        ... )
        >>> parallel_opt = ParallelOptimizer(algorithm, n_jobs=8)
        >>> def objective(params):
        ...     return run_backtest(**params)['sharpe_ratio']
        >>> parallel_opt.run(objective)
        >>> best_params = parallel_opt.get_best_params()
    """
```

#### Architecture Documentation
Add to docs/architecture/optimization.md:
- Parallelization strategies (multiprocessing vs Ray)
- Expected speedup curves
- Overhead analysis
- Best practices for parallel optimization

### Type Hints and Validation
[Source: [coding-standards.md](docs/architecture/coding-standards.md#L9-L25)]
- 100% type hint coverage
- pydantic models for config validation
- mypy --strict compliance

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-02 | 1.0 | Initial story creation | Bob (SM Agent) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent_

### Debug Log References
_To be populated by dev agent_

### Completion Notes List
_To be populated by dev agent_

### File List
_To be populated by dev agent_

## QA Results
_To be populated by QA agent_
