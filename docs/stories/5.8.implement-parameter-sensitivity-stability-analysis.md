# Story 5.8: Implement Parameter Sensitivity and Stability Analysis

## Status
Draft

## Story
**As a** quantitative trader,
**I want** sensitivity analysis showing performance variance across parameter ranges,
**so that** I can identify robust parameters vs. overfit parameters sensitive to small changes.

## Acceptance Criteria
1. SensitivityAnalyzer varies each parameter independently while holding others constant
2. Performance surface visualization (1D/2D plots showing parameter vs. objective function)
3. Stability metric calculated: performance variance across parameter neighborhood
4. Robust parameter identification: parameters with flat performance surface = robust
5. Sensitive parameter flagging: parameters with sharp performance cliffs = likely overfit
6. Interaction analysis: detect parameter interactions (2D heatmaps)
7. Confidence intervals calculated for each parameter (bootstrap or analytical)
8. Report generation with recommendations (prefer parameters in stable regions)
9. Tests validate sensitivity analysis with synthetic functions (known stable/unstable regions)
10. Documentation explains how to interpret sensitivity plots and identify overfitting

## Tasks / Subtasks
- [ ] Implement SensitivityAnalyzer class (AC: 1, 3)
  - [ ] Create rustybt/optimization/sensitivity.py
  - [ ] Define SensitivityAnalyzer class
  - [ ] Implement single-parameter variation (vary one, hold others constant)
  - [ ] Configure base parameters (center point for analysis)
  - [ ] Configure perturbation range (±X% around base value)
  - [ ] Run backtests across parameter range
  - [ ] Calculate stability metric (variance, gradient, curvature)
- [ ] Implement performance surface visualization (AC: 2)
  - [ ] 1D line plots: parameter value vs. objective function
  - [ ] 2D contour plots: two parameters vs. objective (heatmap)
  - [ ] Mark base parameter value on plots
  - [ ] Highlight stable regions (flat surface)
  - [ ] Highlight sensitive regions (steep gradient)
  - [ ] Save plots to configurable output directory
- [ ] Calculate stability metrics (AC: 3, 4, 5)
  - [ ] Variance: std(performance) across neighborhood
  - [ ] Gradient: rate of change (numerical derivative)
  - [ ] Curvature: second derivative (convexity)
  - [ ] Stability score: composite metric (low variance + low gradient = stable)
  - [ ] Classify params: robust (stable) vs. sensitive (unstable)
- [ ] Implement interaction analysis (AC: 6)
  - [ ] 2D grid search over parameter pairs
  - [ ] Generate interaction heatmaps (param1 × param2 vs. objective)
  - [ ] Detect interactions: surface not separable (∂²f/∂x∂y ≠ 0)
  - [ ] Flag interacting parameter pairs
  - [ ] Document interaction implications (optimize jointly, not independently)
- [ ] Calculate confidence intervals (AC: 7)
  - [ ] Bootstrap method: resample backtest results, recalculate sensitivity
  - [ ] Analytical method: if possible (e.g., normal approximation)
  - [ ] 95% confidence intervals for stability metrics
  - [ ] Visualize confidence bands on sensitivity plots
- [ ] Generate analysis report (AC: 8)
  - [ ] Summary table: parameter, stability score, classification
  - [ ] Recommendations: prefer stable regions, avoid cliff edges
  - [ ] Interaction matrix: pairwise interaction strengths
  - [ ] Robustness assessment: overall strategy robustness score
  - [ ] Export report as markdown/HTML
- [ ] Write comprehensive tests (AC: 9)
  - [ ] Test on synthetic stable function (flat surface)
  - [ ] Test on synthetic sensitive function (sharp peaks)
  - [ ] Test interaction detection (known interacting params)
  - [ ] Test stability metric calculation
  - [ ] Test visualization generation
  - [ ] Property test: stability inversely related to gradient
- [ ] Create example notebook (AC: 10)
  - [ ] Create examples/optimization/sensitivity_analysis.ipynb
  - [ ] Demonstrate sensitivity analysis on moving average strategy
  - [ ] Show 1D sensitivity plots for each parameter
  - [ ] Show 2D interaction heatmaps
  - [ ] Interpret stable vs. sensitive parameters
  - [ ] Show how to select robust parameter values
- [ ] Add documentation
  - [ ] Explain sensitivity analysis concept
  - [ ] Document interpretation guide (how to read plots)
  - [ ] Explain overfitting indicators (high sensitivity, sharp cliffs)
  - [ ] Document recommended actions based on analysis
  - [ ] Add usage examples with different strategies

## Dev Notes

### Previous Story Context
[Source: Story 5.1-5.7]
- Optimization finds best parameters
- Walk-forward validates temporal robustness
- Sensitivity analysis validates parameter robustness
- Identifies if optimal params are on a cliff edge (overfitting)

### Relevant Source Tree Info
[Source: [source-tree.md](docs/architecture/source-tree.md#L106-L117)]
File location: `rustybt/optimization/sensitivity.py`

Test location: `tests/optimization/test_sensitivity.py`

Example location: `examples/optimization/sensitivity_analysis.ipynb`

### Sensitivity Analysis Concept
[Source: AC 1]

**Goal**: Determine how sensitive strategy performance is to parameter changes.

**Method**: Vary each parameter independently, measure performance change.

**Interpretation**:
- **Flat surface** (low sensitivity) → Robust parameter, performance stable across range
- **Steep surface** (high sensitivity) → Overfit parameter, performance fragile

### Single-Parameter Variation
[Source: AC 1]

**Process**:
1. Fix base parameters: `lookback=20, threshold=0.02`
2. Vary lookback: `[10, 12, 14, ..., 28, 30]` (threshold fixed at 0.02)
3. Run backtest for each lookback value
4. Plot lookback vs. Sharpe ratio

Repeat for each parameter independently.

### Performance Surface Visualization
[Source: AC 2]

**1D Plot** (single parameter):
```python
import matplotlib.pyplot as plt

plt.plot(param_values, sharpe_ratios)
plt.axvline(base_value, color='r', label='Base')
plt.xlabel('Lookback Period')
plt.ylabel('Sharpe Ratio')
plt.title('Sensitivity Analysis: Lookback Period')
plt.fill_between(param_values, lower_ci, upper_ci, alpha=0.3)
```

**2D Heatmap** (parameter pair):
```python
import seaborn as sns

sns.heatmap(sharpe_matrix, xticklabels=lookback_values, yticklabels=threshold_values)
plt.xlabel('Lookback Period')
plt.ylabel('Threshold')
plt.title('Interaction Analysis: Lookback × Threshold')
```

### Stability Metrics
[Source: AC 3, 4, 5]

**Variance** (simple):
```python
variance = np.var(performance_values)
```

**Gradient** (rate of change):
```python
gradient = np.gradient(performance_values, param_values)
max_gradient = np.max(np.abs(gradient))
```

**Curvature** (convexity):
```python
curvature = np.gradient(gradient, param_values)
max_curvature = np.max(np.abs(curvature))
```

**Stability Score** (composite):
```python
stability_score = 1 / (1 + variance + max_gradient + max_curvature)
# Higher score = more stable
```

**Classification**:
- stability_score > 0.8 → Robust
- 0.5 < stability_score ≤ 0.8 → Moderate
- stability_score ≤ 0.5 → Sensitive (overfit risk)

### Interaction Analysis
[Source: AC 6]

**Detection**: Parameters interact if optimizing one depends on value of the other.

**Method**:
1. 2D grid over parameter pair (param1 × param2)
2. Run backtest for each (param1, param2) combination
3. Generate heatmap
4. Check if surface is separable: f(x,y) ≈ g(x) + h(y)

**Interaction metric**:
```python
# If separable, cross-derivative ≈ 0
cross_derivative = np.gradient(np.gradient(perf_matrix, axis=0), axis=1)
interaction_strength = np.mean(np.abs(cross_derivative))

if interaction_strength > threshold:
    print("Parameters interact - optimize jointly")
```

### Confidence Intervals
[Source: AC 7]

**Bootstrap method**:
```python
def bootstrap_sensitivity(param_range, n_bootstrap=100):
    stability_scores = []
    for _ in range(n_bootstrap):
        # Resample backtest results
        resampled = resample(backtest_results)
        # Recalculate sensitivity
        stability = calculate_stability(resampled, param_range)
        stability_scores.append(stability)

    # 95% CI
    lower = np.percentile(stability_scores, 2.5)
    upper = np.percentile(stability_scores, 97.5)
    return lower, upper
```

### Analysis Report
[Source: AC 8]

**Example report**:
```markdown
# Parameter Sensitivity Analysis

## Summary

| Parameter | Base Value | Stability Score | Classification | Recommendation |
|-----------|------------|-----------------|----------------|----------------|
| lookback  | 20         | 0.85            | Robust         | ✓ Safe to use  |
| threshold | 0.02       | 0.45            | Sensitive      | ⚠ Use caution  |
| rebalance | 5          | 0.90            | Robust         | ✓ Safe to use  |

## Robustness Assessment
Overall strategy robustness: **Moderate** (2/3 parameters robust)

## Recommendations
1. **lookback=20**: Robust parameter, performance stable ±30% range
2. **threshold=0.02**: Sensitive parameter, consider widening or using stable alternative
3. **rebalance=5**: Robust parameter, performance stable ±40% range

## Interactions
- lookback × threshold: **Moderate interaction** (correlation: 0.35)
  → Optimize jointly, not independently
```

### Synthetic Test Functions
[Source: AC 9]

**Stable function** (flat surface):
```python
def stable_sphere(x, y):
    """Broad, flat minimum - parameters robust."""
    return -(0.01*x**2 + 0.01*y**2)  # Very gradual
```

**Sensitive function** (sharp peak):
```python
def sensitive_rastrigin(x, y):
    """Sharp, narrow peaks - parameters sensitive."""
    A = 10
    return -(A*2 + (x**2 - A*np.cos(2*np.pi*x)) + (y**2 - A*np.cos(2*np.pi*y)))
```

**Expected**:
- Stable function → high stability scores, low gradient
- Sensitive function → low stability scores, high gradient

### Interpretation Guide
[Source: AC 10]

**1D Sensitivity Plot**:
- **Flat line** → Robust param, performance insensitive to changes
- **Steep slope** → Sensitive param, small change = big performance impact
- **Cliff edge** → Overfit, optimal param on unstable boundary

**2D Interaction Heatmap**:
- **Diagonal bands** → Interaction present, optimize jointly
- **Horizontal/vertical bands** → No interaction, optimize independently

**Overfitting Indicators**:
1. High sensitivity (steep gradients)
2. Sharp performance cliffs near optimal
3. Low stability scores (<0.5)
4. High variance across neighborhood

### Tech Stack
[Source: [tech-stack.md](docs/architecture/tech-stack.md)]
- numpy for numerical derivatives
- scipy for advanced metrics (optional)
- matplotlib/seaborn for visualization
- scikit-learn for bootstrap resampling

### Testing

#### Test File Location
[Source: [testing-strategy.md](docs/architecture/testing-strategy.md#L441-L444)]
Tests at: `tests/optimization/test_sensitivity.py`

#### Coverage Requirements
[Source: [testing-strategy.md](docs/architecture/testing-strategy.md#L7)]
- **New Components**: ≥90% strict enforcement

#### Synthetic Function Tests
[Source: AC 9]
```python
def test_sensitivity_on_stable_function():
    """Stable function should have high stability scores."""

    def stable_quadratic(params):
        return -(0.1 * params['x']**2)  # Gradual, flat

    analyzer = SensitivityAnalyzer(base_params={'x': 0.0})
    results = analyzer.analyze(
        objective=stable_quadratic,
        param_ranges={'x': (-10, 10)},
        n_points=20
    )

    # Stable function → high stability score
    assert results['x']['stability_score'] > 0.8
    assert results['x']['classification'] == 'robust'


def test_sensitivity_on_sensitive_function():
    """Sensitive function should have low stability scores."""

    def sensitive_peak(params):
        return -np.exp(-100 * params['x']**2)  # Sharp peak

    analyzer = SensitivityAnalyzer(base_params={'x': 0.0})
    results = analyzer.analyze(
        objective=sensitive_peak,
        param_ranges={'x': (-1, 1)},
        n_points=20
    )

    # Sensitive function → low stability score
    assert results['x']['stability_score'] < 0.5
    assert results['x']['classification'] == 'sensitive'
```

#### Property Tests
[Source: [testing-strategy.md](docs/architecture/testing-strategy.md#L26-L85)]
```python
@given(
    variance=st.floats(min_value=0.0, max_value=10.0),
    gradient=st.floats(min_value=0.0, max_value=10.0)
)
def test_stability_score_invariant(variance, gradient):
    """Higher variance/gradient should decrease stability score."""
    score1 = calculate_stability_score(variance=variance, gradient=gradient)
    score2 = calculate_stability_score(variance=variance*2, gradient=gradient*2)

    # Double variance/gradient → lower stability
    assert score2 < score1
```

#### Zero-Mock Enforcement
[Source: [coding-standards.md](docs/architecture/coding-standards.md#L137-L212)]
- No mocked sensitivity calculations
- Tests use real numerical derivatives
- Validation tests run actual backtests on synthetic functions

### Documentation

#### Docstring Example
[Source: [coding-standards.md](docs/architecture/coding-standards.md#L48-L74)]
```python
class SensitivityAnalyzer:
    """Parameter sensitivity and stability analysis for strategy robustness.

    Identifies robust parameters (flat performance surface) vs. sensitive
    parameters (sharp performance cliffs). Detects parameter interactions.

    Best for:
        - Validating optimization results
        - Identifying overfitting to specific parameter values
        - Understanding parameter importance
        - Selecting robust parameter regions

    Args:
        base_params: Center point for sensitivity analysis
        n_points: Points to sample per parameter (default: 20)
        perturbation_pct: Range to vary params as % of base (default: 0.5 = ±50%)

    Example:
        >>> analyzer = SensitivityAnalyzer(
        ...     base_params={'lookback': 20, 'threshold': 0.02},
        ...     n_points=20,
        ...     perturbation_pct=0.5
        ... )
        >>> results = analyzer.analyze(objective=run_backtest, param_ranges=...)
        >>> print(results['lookback']['stability_score'])  # 0.85 (robust)
        >>> print(results['threshold']['stability_score']) # 0.45 (sensitive)
        >>> analyzer.plot_sensitivity('lookback')
        >>> analyzer.plot_interaction('lookback', 'threshold')
        >>> report = analyzer.generate_report()
    """
```

#### Architecture Documentation
Add to docs/architecture/optimization.md:
- Sensitivity analysis concept and benefits
- Interpretation guide for plots
- Overfitting detection using sensitivity
- Robust parameter selection guidelines

### Type Hints and Validation
[Source: [coding-standards.md](docs/architecture/coding-standards.md#L9-L25)]
- 100% type hint coverage
- pydantic models for config validation
- mypy --strict compliance

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-02 | 1.0 | Initial story creation | Bob (SM Agent) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent_

### Debug Log References
_To be populated by dev agent_

### Completion Notes List
_To be populated by dev agent_

### File List
_To be populated by dev agent_

## QA Results
_To be populated by QA agent_
