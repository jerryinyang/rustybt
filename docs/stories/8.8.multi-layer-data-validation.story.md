# Story 8.8: Multi-Layer Data Validation

## Status
Ready for development

## Story
**As a** quantitative trader,
**I want** comprehensive data validation preventing invalid data from causing errors,
**so that** I can trust data quality throughout the system.

## Acceptance Criteria
1. Layer 1 - Schema validation: correct types, required fields, value ranges (Pydantic models)
2. Layer 2 - OHLCV relationship validation: High ≥ Low, High ≥ Open/Close, Low ≤ Open/Close, Volume ≥ 0
3. Layer 3 - Outlier detection: price spikes, volume anomalies flagged for review
4. Layer 4 - Temporal consistency: timestamps sorted, no duplicates, no future data, gap detection
5. Validation runs on data ingestion (prevent bad data from entering catalog)
6. Validation runs on strategy execution (detect corrupted data before causing errors)
7. Validation errors logged with severity (ERROR for critical, WARN for suspicious)
8. Validation configurable (thresholds adjustable per asset class)
9. Tests validate each validation layer with synthetic bad data
10. Documentation explains validation layers and configuration

## Tasks / Subtasks
- [ ] Create DataValidator class (AC: 1-8)
  - [ ] Implement `__init__(config: ValidationConfig)`
  - [ ] Implement `validate(data: pl.DataFrame, layer: str = 'all')` method
  - [ ] Support validating individual layers or all layers
- [ ] Implement Layer 1: Schema validation (AC: 1)
  - [ ] Use Pydantic models to validate data schema
  - [ ] Check required columns exist (open, high, low, close, volume, timestamp)
  - [ ] Check data types (Decimal for prices, int for volume, Timestamp for dates)
  - [ ] Check value ranges (prices > 0, volume >= 0)
- [ ] Implement Layer 2: OHLCV relationship validation (AC: 2)
  - [ ] Validate: high >= low (all bars)
  - [ ] Validate: high >= open (all bars)
  - [ ] Validate: high >= close (all bars)
  - [ ] Validate: low <= open (all bars)
  - [ ] Validate: low <= close (all bars)
  - [ ] Validate: volume >= 0 (all bars)
  - [ ] Report all violations with row numbers
- [ ] Implement Layer 3: Outlier detection (AC: 3)
  - [ ] Detect price spikes: price change > N * historical volatility
  - [ ] Detect volume anomalies: volume > M * average volume
  - [ ] Use configurable thresholds (e.g., N=5 std deviations, M=10x average)
  - [ ] Flag outliers as WARNING (not ERROR, as outliers may be legitimate)
- [ ] Implement Layer 4: Temporal consistency (AC: 4)
  - [ ] Validate timestamps are sorted ascending
  - [ ] Detect duplicate timestamps
  - [ ] Detect future data (timestamp > current time)
  - [ ] Detect gaps (missing expected bars based on frequency)
  - [ ] Report temporal violations
- [ ] Integrate validation into data ingestion (AC: 5)
  - [ ] Add validation to data adapters (CCXTAdapter, YFinanceAdapter, etc.)
  - [ ] Run validation before writing to catalog
  - [ ] Reject bad data with clear error messages
  - [ ] Log validation failures
- [ ] Integrate validation into strategy execution (AC: 6)
  - [ ] Add validation to DataPortal when fetching data
  - [ ] Run lightweight validation (schema + OHLCV relationships) on every data fetch
  - [ ] Skip expensive validation (outlier detection) during strategy execution
  - [ ] Raise DataValidationError if corrupted data detected
- [ ] Implement validation logging (AC: 7)
  - [ ] Log validation failures with severity
  - [ ] ERROR: Critical violations (schema errors, OHLCV violations, future data)
  - [ ] WARNING: Suspicious but potentially valid (outliers, gaps)
  - [ ] Include context: asset, timestamp, violation type, value
- [ ] Make validation configurable (AC: 8)
  - [ ] Create ValidationConfig dataclass
  - [ ] Support per-asset-class thresholds (stocks, crypto, futures have different volatility)
  - [ ] Support disabling specific validation layers (for testing/debugging)
  - [ ] Load config from file or environment variables
- [ ] Write tests (AC: 9)
  - [ ] Unit test: Schema validation with invalid data
  - [ ] Unit test: OHLCV relationship validation with violation
  - [ ] Unit test: Outlier detection with synthetic spike
  - [ ] Unit test: Temporal consistency with duplicates, gaps, future data
  - [ ] Integration test: Full validation on synthetic bad dataset
- [ ] Write documentation (AC: 10)
  - [ ] Document validation layers and what each checks
  - [ ] Document configuration options and defaults
  - [ ] Provide examples: "How to configure validation for crypto"
  - [ ] Explain how to interpret validation errors

## Dev Notes

### Relevant Source Tree
[Source: docs/architecture/source-tree.md]
- DataValidator: `rustybt/utils/validation.py` (extend existing, or create data_validation.py)
- Update: `rustybt/data/adapters/base.py` (add validation to fetch methods)
- Update: `rustybt/data/polars/data_portal.py` (add validation to data access)

### Tech Stack
[Source: docs/architecture/tech-stack.md]
- **Validation**: **pydantic** (schema validation)
- **DataFrames**: **polars** (data manipulation)
- **Statistics**: **scipy** (outlier detection using z-scores)

### Validation Layers

**Layer 1: Schema Validation (Pydantic)**
```python
from pydantic import BaseModel, Field, validator
from decimal import Decimal
from datetime import datetime

class OHLCVBar(BaseModel):
    """Single OHLCV bar schema."""
    timestamp: datetime
    open: Decimal = Field(..., gt=0)
    high: Decimal = Field(..., gt=0)
    low: Decimal = Field(..., gt=0)
    close: Decimal = Field(..., gt=0)
    volume: Decimal = Field(..., ge=0)
    
    @validator('high')
    def high_ge_low(cls, v, values):
        if 'low' in values and v < values['low']:
            raise ValueError(f"high ({v}) must be >= low ({values['low']})")
        return v
```

**Layer 2: OHLCV Relationship Validation**
```python
def validate_ohlcv_relationships(data: pl.DataFrame) -> list[str]:
    """Validate OHLCV bar relationships."""
    violations = []
    
    # High >= Low
    invalid_high_low = data.filter(pl.col('high') < pl.col('low'))
    if len(invalid_high_low) > 0:
        violations.append(f"High < Low in {len(invalid_high_low)} bars")
    
    # High >= Open
    invalid_high_open = data.filter(pl.col('high') < pl.col('open'))
    if len(invalid_high_open) > 0:
        violations.append(f"High < Open in {len(invalid_high_open)} bars")
    
    # High >= Close
    invalid_high_close = data.filter(pl.col('high') < pl.col('close'))
    if len(invalid_high_close) > 0:
        violations.append(f"High < Close in {len(invalid_high_close)} bars")
    
    # Low <= Open
    invalid_low_open = data.filter(pl.col('low') > pl.col('open'))
    if len(invalid_low_open) > 0:
        violations.append(f"Low > Open in {len(invalid_low_open)} bars")
    
    # Low <= Close
    invalid_low_close = data.filter(pl.col('low') > pl.col('close'))
    if len(invalid_low_close) > 0:
        violations.append(f"Low > Close in {len(invalid_low_close)} bars")
    
    # Volume >= 0
    invalid_volume = data.filter(pl.col('volume') < 0)
    if len(invalid_volume) > 0:
        violations.append(f"Volume < 0 in {len(invalid_volume)} bars")
    
    return violations
```

**Layer 3: Outlier Detection**
```python
def detect_outliers(data: pl.DataFrame, threshold_std: float = 5.0) -> dict:
    """Detect price and volume outliers."""
    outliers = {'price_spikes': [], 'volume_spikes': []}
    
    # Calculate returns
    data = data.with_columns([
        (pl.col('close') / pl.col('close').shift(1) - 1).alias('return')
    ])
    
    # Price spikes (return > N std deviations)
    mean_return = data['return'].mean()
    std_return = data['return'].std()
    price_spikes = data.filter(
        abs(pl.col('return') - mean_return) > threshold_std * std_return
    )
    outliers['price_spikes'] = price_spikes.select(['timestamp', 'close', 'return']).to_dicts()
    
    # Volume spikes (volume > N * average volume)
    mean_volume = data['volume'].mean()
    volume_spikes = data.filter(
        pl.col('volume') > threshold_std * mean_volume
    )
    outliers['volume_spikes'] = volume_spikes.select(['timestamp', 'volume']).to_dicts()
    
    return outliers
```

**Layer 4: Temporal Consistency**
```python
def validate_temporal_consistency(data: pl.DataFrame, expected_frequency: str = '1d') -> list[str]:
    """Validate timestamp consistency."""
    violations = []
    
    # Check timestamps are sorted
    timestamps = data['timestamp'].to_list()
    if timestamps != sorted(timestamps):
        violations.append("Timestamps not sorted")
    
    # Check for duplicates
    duplicates = data.groupby('timestamp').agg(pl.count()).filter(pl.col('count') > 1)
    if len(duplicates) > 0:
        violations.append(f"Duplicate timestamps: {len(duplicates)}")
    
    # Check for future data
    now = pd.Timestamp.now(tz='UTC')
    future_data = data.filter(pl.col('timestamp') > now)
    if len(future_data) > 0:
        violations.append(f"Future data detected: {len(future_data)} bars")
    
    # Check for gaps (simplified - assumes daily data)
    if expected_frequency == '1d':
        expected_delta = pd.Timedelta(days=1)
        for i in range(1, len(timestamps)):
            actual_delta = timestamps[i] - timestamps[i-1]
            if actual_delta > expected_delta * 3:  # Allow for weekends
                violations.append(f"Gap detected: {timestamps[i-1]} to {timestamps[i]}")
    
    return violations
```

### Configuration
```python
from dataclasses import dataclass
from typing import Optional

@dataclass
class ValidationConfig:
    """Configuration for data validation."""
    
    # Layer 1: Schema validation
    enforce_schema: bool = True
    
    # Layer 2: OHLCV relationship validation
    enforce_ohlcv_relationships: bool = True
    
    # Layer 3: Outlier detection
    enable_outlier_detection: bool = True
    price_spike_threshold_std: float = 5.0  # std deviations
    volume_spike_threshold: float = 10.0    # x average volume
    
    # Layer 4: Temporal consistency
    enforce_temporal_consistency: bool = True
    allow_gaps: bool = True  # Allow gaps in data (e.g., market holidays)
    max_gap_days: int = 7    # Maximum allowed gap
    
    # Asset-class specific overrides
    crypto_config: Optional['ValidationConfig'] = None  # More lenient for 24/7 markets
```

### Testing
[Source: docs/architecture/testing-strategy.md]
- **Test Location**: `tests/utils/test_data_validation.py`
- **Test Types**:
  - Unit tests: Each validation layer with synthetic bad data
  - Property tests: Valid data passes all layers
  - Integration tests: Full validation on real dataset
- **Coverage Target**: ≥90%

### Zero-Mock Enforcement
[Source: docs/architecture/coding-standards.md#zero-mock-enforcement-mandatory]
- Validation must perform actual checks, not return hardcoded results
- Tests must use actual invalid data, not mocked validation failures
- Outlier detection must calculate real statistics, not return fake outliers

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-31 | 1.0 | Initial story draft | Bob (Scrum Master) |

## Dev Agent Record
_This section will be populated by the development agent during implementation._

### Agent Model Used
_Not yet populated_

### Debug Log References
_Not yet populated_

### Completion Notes List
_Not yet populated_

### File List
_Not yet populated_

## QA Results
_This section will be populated by the QA Agent after story completion._
