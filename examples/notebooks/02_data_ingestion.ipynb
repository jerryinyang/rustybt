{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion with RustyBT\n",
    "\n",
    "This notebook demonstrates how to fetch and prepare data from multiple sources for backtesting.\n",
    "\n",
    "**Data Sources Covered:**\n",
    "- yfinance (Yahoo Finance) - Free stocks, ETFs, forex\n",
    "- CCXT - Cryptocurrency exchanges (100+ exchanges)\n",
    "- CSV files - Custom data\n",
    "- Alpaca - Real-time and historical market data\n",
    "\n",
    "**What you'll learn:**\n",
    "- Fetching data from different providers\n",
    "- Data validation and quality checks\n",
    "- Creating custom data bundles\n",
    "- Caching for performance\n",
    "\n",
    "**Estimated runtime:** 5-10 minutes (depending on data downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from rustybt.analytics import setup_notebook, create_progress_iterator\n",
    "setup_notebook()\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from rustybt.data.adapters import YFinanceAdapter, CCXTAdapter, CSVAdapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Yahoo Finance Data (Stocks & ETFs)\n",
    "\n",
    "Yahoo Finance provides free historical data for stocks, ETFs, indices, and forex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize yfinance adapter\n",
    "yf_adapter = YFinanceAdapter()\n",
    "\n",
    "# Fetch data for multiple stocks\n",
    "symbols = ['AAPL', 'GOOGL', 'MSFT', 'TSLA']\n",
    "start_date = pd.Timestamp('2023-01-01')\n",
    "end_date = pd.Timestamp('2023-12-31')\n",
    "\n",
    "print(f\"Fetching data for {len(symbols)} symbols...\")\n",
    "\n",
    "# Fetch with progress bar\n",
    "all_data = []\n",
    "for symbol in create_progress_iterator(symbols, desc=\"Downloading\"):\n",
    "    data = yf_adapter.fetch(\n",
    "        symbols=[symbol],\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        resolution='1d'\n",
    "    )\n",
    "    all_data.append(data)\n",
    "    print(f\"  {symbol}: {len(data)} bars\")\n",
    "\n",
    "# Combine all data\n",
    "combined = pl.concat(all_data)\n",
    "print(f\"\\nTotal: {len(combined)} bars across {len(symbols)} symbols\")\n",
    "print(f\"\\nData schema:\")\n",
    "print(combined.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data quality\n",
    "try:\n",
    "    yf_adapter.validate(combined)\n",
    "    print(\"‚úÖ Data validation passed!\")\n",
    "    print(\"   - All OHLCV relationships valid\")\n",
    "    print(\"   - No NULL values\")\n",
    "    print(\"   - Timestamps properly sorted\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cryptocurrency Data (CCXT)\n",
    "\n",
    "CCXT provides unified access to 100+ cryptocurrency exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CCXT adapter for Binance\n",
    "binance = CCXTAdapter(exchange_id='binance')\n",
    "\n",
    "# Fetch BTC and ETH data\n",
    "crypto_symbols = ['BTC/USDT', 'ETH/USDT']\n",
    "\n",
    "print(f\"Fetching crypto data from Binance...\")\n",
    "\n",
    "crypto_data = []\n",
    "for symbol in crypto_symbols:\n",
    "    data = binance.fetch(\n",
    "        symbols=[symbol],\n",
    "        start_date=pd.Timestamp('2024-01-01'),\n",
    "        end_date=pd.Timestamp('2024-01-31'),\n",
    "        resolution='1h'  # Hourly data\n",
    "    )\n",
    "    crypto_data.append(data)\n",
    "    print(f\"  {symbol}: {len(data)} bars\")\n",
    "\n",
    "crypto_combined = pl.concat(crypto_data)\n",
    "print(f\"\\nTotal crypto bars: {len(crypto_combined)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CSV Data Import\n",
    "\n",
    "Import custom data from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example CSV structure:\n",
    "csv_example = pd.DataFrame({\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=100, freq='D'),\n",
    "    'symbol': 'CUSTOM',\n",
    "    'open': 100 + pd.np.random.randn(100).cumsum(),\n",
    "    'high': 105 + pd.np.random.randn(100).cumsum(),\n",
    "    'low': 95 + pd.np.random.randn(100).cumsum(),\n",
    "    'close': 100 + pd.np.random.randn(100).cumsum(),\n",
    "    'volume': pd.np.random.randint(1000000, 10000000, 100)\n",
    "})\n",
    "\n",
    "# Save example CSV\n",
    "csv_example.to_csv('example_data.csv', index=False)\n",
    "\n",
    "# Load using CSV adapter\n",
    "csv_adapter = CSVAdapter()\n",
    "csv_data = csv_adapter.load('example_data.csv')\n",
    "\n",
    "print(f\"Loaded {len(csv_data)} bars from CSV\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(csv_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Checks\n",
    "\n",
    "Always validate data before using in backtests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(df, name=\"Data\"):\n",
    "    \"\"\"Comprehensive data quality check.\"\"\"\n",
    "    print(f\"\\n{name} Quality Report:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check for nulls\n",
    "    null_counts = df.null_count()\n",
    "    if null_counts.sum().sum() > 0:\n",
    "        print(f\"‚ö†Ô∏è  NULL values found:\")\n",
    "        print(null_counts)\n",
    "    else:\n",
    "        print(\"‚úÖ No NULL values\")\n",
    "    \n",
    "    # Check OHLC relationships\n",
    "    invalid = df.filter(\n",
    "        (pl.col('high') < pl.col('low')) |\n",
    "        (pl.col('high') < pl.col('open')) |\n",
    "        (pl.col('high') < pl.col('close')) |\n",
    "        (pl.col('low') > pl.col('open')) |\n",
    "        (pl.col('low') > pl.col('close'))\n",
    "    )\n",
    "    \n",
    "    if len(invalid) > 0:\n",
    "        print(f\"‚ùå Invalid OHLC relationships: {len(invalid)} bars\")\n",
    "    else:\n",
    "        print(\"‚úÖ OHLC relationships valid\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.filter(pl.col('timestamp').is_duplicated())\n",
    "    if len(duplicates) > 0:\n",
    "        print(f\"‚ö†Ô∏è  Duplicate timestamps: {len(duplicates)}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No duplicate timestamps\")\n",
    "    \n",
    "    # Date range\n",
    "    print(f\"\\nüìÖ Date Range:\")\n",
    "    print(f\"   Start: {df['timestamp'].min()}\")\n",
    "    print(f\"   End: {df['timestamp'].max()}\")\n",
    "    print(f\"   Total bars: {len(df)}\")\n",
    "\n",
    "# Check quality\n",
    "check_data_quality(combined, \"Stock Data\")\n",
    "check_data_quality(crypto_combined, \"Crypto Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Data for Backtesting\n",
    "\n",
    "Save data in efficient formats for fast backtesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Parquet (recommended - fast and efficient)\n",
    "combined.write_parquet('stocks_2023.parquet')\n",
    "crypto_combined.write_parquet('crypto_2024_01.parquet')\n",
    "\n",
    "print(\"‚úÖ Data saved to Parquet files\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - stocks_2023.parquet\")\n",
    "print(\"  - crypto_2024_01.parquet\")\n",
    "\n",
    "# Can also save to CSV for compatibility\n",
    "# combined.write_csv('stocks_2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Caching\n",
    "\n",
    "RustyBT supports caching to avoid re-downloading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rustybt.data.catalog import DataCatalog\n",
    "\n",
    "# Initialize catalog with caching\n",
    "catalog = DataCatalog(cache_dir='./data_cache')\n",
    "\n",
    "# Register data source\n",
    "catalog.register(\n",
    "    name='stocks_2023',\n",
    "    adapter=yf_adapter,\n",
    "    symbols=['AAPL', 'GOOGL', 'MSFT'],\n",
    "    start_date=pd.Timestamp('2023-01-01'),\n",
    "    end_date=pd.Timestamp('2023-12-31')\n",
    ")\n",
    "\n",
    "# First call downloads data\n",
    "data1 = catalog.load('stocks_2023')\n",
    "print(f\"First load: {len(data1)} bars (downloaded)\")\n",
    "\n",
    "# Second call uses cache\n",
    "data2 = catalog.load('stocks_2023')\n",
    "print(f\"Second load: {len(data2)} bars (from cache - instant!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have data:\n",
    "\n",
    "1. **03_strategy_development.ipynb** - Build trading strategies with this data\n",
    "2. **10_full_workflow.ipynb** - See complete workflow from data to results\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- ‚úÖ Multiple data sources supported (stocks, crypto, custom)\n",
    "- ‚úÖ Built-in data validation catches errors early\n",
    "- ‚úÖ Efficient Parquet storage for fast backtests\n",
    "- ‚úÖ Caching prevents redundant downloads\n",
    "- ‚úÖ Progress bars for long downloads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
